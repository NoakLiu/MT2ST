D:\Anaconda\exe\python.exe "E:\2023 Fall\23 Fall MT2ST WE Acc Meng\MT2ST-Word-Embeddings-Acceleration-Method-main\MT2ST-Word-Embeddings-Acceleration-Method-main\training_cutoff.py"
Size of the dataset: 1000
["This film did entertain me with lots of laughs at the actors who kept the film moving along in all types of crazy directions. If you like suggestive language and sexy looking gals they were all in the picture and gals and guys all looking burned out before they even graduate from high school. There is one scene where the teenagers drive their car into a very fake deer and then proceed to throw it out into a lake or ocean, which is repeated over and over again. There is no horror to this film except the word Horrible for the entire picture and Arnold who plays a plastic cop is really one sick character. Please don't waste your time viewing this film.", 'The animation in this re-imagining of Peter & the Wolf is excellent, but at 29 minutes, the film is sleep inducing. They should have called it "Peter & the Snails", because everything moves at a snail\'s pace. I couldn\'t even watch the film in one sitting - I had to watch it 15 minutes at a time, and it was pure torture.<br /><br />Save yourself 30 minutes - do not watch this film - and you will thank me.<br /><br />I can only guess that the Oscar nominating committee only watched the first few minutes of the nominees. Unfortunately, to vote for the winner in the Best Animated Short (short!) category, the voters will have to sit through the whole thing. I already feel sorry for them - and must predict that there\'s no way this film will come close to winning.', "This is a great show, and will make you cry, this group people really loved each other in real life and it shows time and time again. Email me and let's chat. I have been to Australia and they real do talk like this.<br /><br />I want you to enjoy Five Mile Creek and pass on these great stories of right and wrong, and friendship to your kids. I have all 40 Episodes on DVD-R that I have collected over the last 5 years. See my Five Mile Creek tribute at www.mikeandvicki.com and hear the extended theme music. Let's talk about them.<br /><br />These people are so cool!", 'This is a great TV miniseries of a classic novel. Janet McTeer and John Bowe, in the lead roles, are exceptional. This is one of the best adaptations from a book that I have seen. I would LOVE to get a copy of this - let me know if you know how I might get one...Thanks!', "My husband dragged me to this film as I had no interest in seeing some Anime cartoon. I was absolutely delighted by the simple story and amazing animation. In a digital world where effects are computer generated it was refreshing to see gorgeous, imaginative hand drawn animation. The world of Sosuke and Ponyo is a vivid fantasyland intermixed with minimal reality. I haven't seen animation like this since I was a child and it is wonderful to see it endure and succeed.<br /><br />The actors supplying the voices in the English version were fabulous. The length of the movie was PERFECT, especially for children who tend to get squirrelly in films. Overall a delightful experience worth the very expensive ticket prices we have nowadays.", "This film was reeeeeeallyyyy bad! Was it meant to be a comedy as I couldn't help laughing the whole way through it? what a waste of two hours! Donald Sutherland was wooden not that he was alone, everyone else was just as bad...and how miscast was linda hamilton???", "If you have enjoyed the Butterfly Effect, Donnie Darko or The Machinist, you will enjoy K-Pax too.<br /><br />To me, this movie felt really uplifting and yet depressing in the end. Spacey delivers a great performance as Prot. Also, lets not forget the appearance of Saul Williams in the movie, who i am a big fan of.<br /><br />After watching it, i recommended the movie to lots of my friends, and everyone was pretty much blown away.<br /><br />But still, it is very underrated, maybe because of the lack of action and explosions. I'm sorry, this is not a movie about blowing things up, it's about how humans behave, and how people live in worlds that don't exist.<br /><br />Go on, and enjoy.", "The first hour or so of the movie was mostly boring to say the least. However it improved afterwards as the Valentine Party commenced. Apart from the twist as to the identity of the killer in the very end, the hot bath murder scene was one of the few relatively memorable aspects of this movie. The scene at the garden with Kate was well shot and so was the very last scene (the 'twist'). In those scenes, there was some genuine suspense and thrills and the hot bath murder scene had a nasty (the way slashers should be) edge to it. The earlier murders are frustratingly devoid of gore.", "As an avid fan of Cary Grant, I expected to watch this movie and howl with laughter, as AMC billed it as a comedy. I have never been more disappointed with a film! Cary's usual charm and effortless comedy are AWOL from this entire movie; he comes across as strained, bored, and just not himself. Mississip's character ranks among one of the worst stereotypes I have ever witnessed - his accent is terribly exaggerated (and incorrect, according to which part of Mississippi he claims to hail from), and whenever he does deliver a line, it's several decibels higher than any other cast member. Mississip tried to make himself stand out in the film as a lovable, country-bumpkin goofball, but in the end, he manages only to detract from the already weak plot. Mansfield looks more like an obscene blow-up doll than a Hollywood sex kitten, and while she was never known in Hollywood for her acting ability, this film screams that she never had that ability to begin with. Ray Walston's character was sugary and ultimately contrived. For four men on shore leave, it was the tamest leave I've ever seen. I watched this nightmare until its very end, and while I won't spoil that for anyone, I will tell you that it's the most absurd you'll ever see. The film tries to spark patriotism and a sense of debt to the fighting men, but the film misses that point totally because of its weak plot line and weak cast. Sorry, Cary!"]
['negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative']
['this film did entertain me with lots of laughs at the actors who kept the film moving along in all types of crazy directions if you like suggestive language and sexy looking gals they were all in the picture and gals and guys all looking burned out before they even graduate from high school there is one scene where the teenagers drive their car into a very fake deer and then proceed to throw it out into a lake or ocean which is repeated over and over again there is no horror to this film except the word horrible for the entire picture and arnold who plays a plastic cop is really one sick character please dont waste your time viewing this film', 'the animation in this reimagining of peter  the wolf is excellent but at 29 minutes the film is sleep inducing they should have called it peter  the snails because everything moves at a snails pace i couldnt even watch the film in one sitting  i had to watch it 15 minutes at a time and it was pure torturesave yourself 30 minutes  do not watch this film  and you will thank mei can only guess that the oscar nominating committee only watched the first few minutes of the nominees unfortunately to vote for the winner in the best animated short short category the voters will have to sit through the whole thing i already feel sorry for them  and must predict that theres no way this film will come close to winning', 'this is a great show and will make you cry this group people really loved each other in real life and it shows time and time again email me and lets chat i have been to australia and they real do talk like thisi want you to enjoy five mile creek and pass on these great stories of right and wrong and friendship to your kids i have all 40 episodes on dvdr that i have collected over the last 5 years see my five mile creek tribute at wwwmikeandvickicom and hear the extended theme music lets talk about themthese people are so cool', 'this is a great tv miniseries of a classic novel janet mcteer and john bowe in the lead roles are exceptional this is one of the best adaptations from a book that i have seen i would love to get a copy of this  let me know if you know how i might get onethanks', 'my husband dragged me to this film as i had no interest in seeing some anime cartoon i was absolutely delighted by the simple story and amazing animation in a digital world where effects are computer generated it was refreshing to see gorgeous imaginative hand drawn animation the world of sosuke and ponyo is a vivid fantasyland intermixed with minimal reality i havent seen animation like this since i was a child and it is wonderful to see it endure and succeedthe actors supplying the voices in the english version were fabulous the length of the movie was perfect especially for children who tend to get squirrelly in films overall a delightful experience worth the very expensive ticket prices we have nowadays', 'this film was reeeeeeallyyyy bad was it meant to be a comedy as i couldnt help laughing the whole way through it what a waste of two hours donald sutherland was wooden not that he was alone everyone else was just as badand how miscast was linda hamilton', 'if you have enjoyed the butterfly effect donnie darko or the machinist you will enjoy kpax tooto me this movie felt really uplifting and yet depressing in the end spacey delivers a great performance as prot also lets not forget the appearance of saul williams in the movie who i am a big fan ofafter watching it i recommended the movie to lots of my friends and everyone was pretty much blown awaybut still it is very underrated maybe because of the lack of action and explosions im sorry this is not a movie about blowing things up its about how humans behave and how people live in worlds that dont existgo on and enjoy', 'the first hour or so of the movie was mostly boring to say the least however it improved afterwards as the valentine party commenced apart from the twist as to the identity of the killer in the very end the hot bath murder scene was one of the few relatively memorable aspects of this movie the scene at the garden with kate was well shot and so was the very last scene the twist in those scenes there was some genuine suspense and thrills and the hot bath murder scene had a nasty the way slashers should be edge to it the earlier murders are frustratingly devoid of gore', 'as an avid fan of cary grant i expected to watch this movie and howl with laughter as amc billed it as a comedy i have never been more disappointed with a film carys usual charm and effortless comedy are awol from this entire movie he comes across as strained bored and just not himself mississips character ranks among one of the worst stereotypes i have ever witnessed  his accent is terribly exaggerated and incorrect according to which part of mississippi he claims to hail from and whenever he does deliver a line its several decibels higher than any other cast member mississip tried to make himself stand out in the film as a lovable countrybumpkin goofball but in the end he manages only to detract from the already weak plot mansfield looks more like an obscene blowup doll than a hollywood sex kitten and while she was never known in hollywood for her acting ability this film screams that she never had that ability to begin with ray walstons character was sugary and ultimately contrived for four men on shore leave it was the tamest leave ive ever seen i watched this nightmare until its very end and while i wont spoil that for anyone i will tell you that its the most absurd youll ever see the film tries to spark patriotism and a sense of debt to the fighting men but the film misses that point totally because of its weak plot line and weak cast sorry cary']
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\LdTenacity\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[['this', 'film', 'did', 'entertain', 'me', 'with', 'lots', 'of', 'laughs', 'at', 'the', 'actors', 'who', 'kept', 'the', 'film', 'moving', 'along', 'in', 'all', 'types', 'of', 'crazy', 'directions', 'if', 'you', 'like', 'suggestive', 'language', 'and', 'sexy', 'looking', 'gals', 'they', 'were', 'all', 'in', 'the', 'picture', 'and', 'gals', 'and', 'guys', 'all', 'looking', 'burned', 'out', 'before', 'they', 'even', 'graduate', 'from', 'high', 'school', 'there', 'is', 'one', 'scene', 'where', 'the', 'teenagers', 'drive', 'their', 'car', 'into', 'a', 'very', 'fake', 'deer', 'and', 'then', 'proceed', 'to', 'throw', 'it', 'out', 'into', 'a', 'lake', 'or', 'ocean', 'which', 'is', 'repeated', 'over', 'and', 'over', 'again', 'there', 'is', 'no', 'horror', 'to', 'this', 'film', 'except', 'the', 'word', 'horrible', 'for', 'the', 'entire', 'picture', 'and', 'arnold', 'who', 'plays', 'a', 'plastic', 'cop', 'is', 'really', 'one', 'sick', 'character', 'please', 'dont', 'waste', 'your', 'time', 'viewing', 'this', 'film'], ['the', 'animation', 'in', 'this', 'reimagining', 'of', 'peter', 'the', 'wolf', 'is', 'excellent', 'but', 'at', '29', 'minutes', 'the', 'film', 'is', 'sleep', 'inducing', 'they', 'should', 'have', 'called', 'it', 'peter', 'the', 'snails', 'because', 'everything', 'moves', 'at', 'a', 'snails', 'pace', 'i', 'couldnt', 'even', 'watch', 'the', 'film', 'in', 'one', 'sitting', 'i', 'had', 'to', 'watch', 'it', '15', 'minutes', 'at', 'a', 'time', 'and', 'it', 'was', 'pure', 'torturesave', 'yourself', '30', 'minutes', 'do', 'not', 'watch', 'this', 'film', 'and', 'you', 'will', 'thank', 'mei', 'can', 'only', 'guess', 'that', 'the', 'oscar', 'nominating', 'committee', 'only', 'watched', 'the', 'first', 'few', 'minutes', 'of', 'the', 'nominees', 'unfortunately', 'to', 'vote', 'for', 'the', 'winner', 'in', 'the', 'best', 'animated', 'short', 'short', 'category', 'the', 'voters', 'will', 'have', 'to', 'sit', 'through', 'the', 'whole', 'thing', 'i', 'already', 'feel', 'sorry', 'for', 'them', 'and', 'must', 'predict', 'that', 'theres', 'no', 'way', 'this', 'film', 'will', 'come', 'close', 'to', 'winning'], ['this', 'is', 'a', 'great', 'show', 'and', 'will', 'make', 'you', 'cry', 'this', 'group', 'people', 'really', 'loved', 'each', 'other', 'in', 'real', 'life', 'and', 'it', 'shows', 'time', 'and', 'time', 'again', 'email', 'me', 'and', 'lets', 'chat', 'i', 'have', 'been', 'to', 'australia', 'and', 'they', 'real', 'do', 'talk', 'like', 'thisi', 'want', 'you', 'to', 'enjoy', 'five', 'mile', 'creek', 'and', 'pass', 'on', 'these', 'great', 'stories', 'of', 'right', 'and', 'wrong', 'and', 'friendship', 'to', 'your', 'kids', 'i', 'have', 'all', '40', 'episodes', 'on', 'dvdr', 'that', 'i', 'have', 'collected', 'over', 'the', 'last', '5', 'years', 'see', 'my', 'five', 'mile', 'creek', 'tribute', 'at', 'wwwmikeandvickicom', 'and', 'hear', 'the', 'extended', 'theme', 'music', 'lets', 'talk', 'about', 'themthese', 'people', 'are', 'so', 'cool'], ['this', 'is', 'a', 'great', 'tv', 'miniseries', 'of', 'a', 'classic', 'novel', 'janet', 'mcteer', 'and', 'john', 'bowe', 'in', 'the', 'lead', 'roles', 'are', 'exceptional', 'this', 'is', 'one', 'of', 'the', 'best', 'adaptations', 'from', 'a', 'book', 'that', 'i', 'have', 'seen', 'i', 'would', 'love', 'to', 'get', 'a', 'copy', 'of', 'this', 'let', 'me', 'know', 'if', 'you', 'know', 'how', 'i', 'might', 'get', 'onethanks'], ['my', 'husband', 'dragged', 'me', 'to', 'this', 'film', 'as', 'i', 'had', 'no', 'interest', 'in', 'seeing', 'some', 'anime', 'cartoon', 'i', 'was', 'absolutely', 'delighted', 'by', 'the', 'simple', 'story', 'and', 'amazing', 'animation', 'in', 'a', 'digital', 'world', 'where', 'effects', 'are', 'computer', 'generated', 'it', 'was', 'refreshing', 'to', 'see', 'gorgeous', 'imaginative', 'hand', 'drawn', 'animation', 'the', 'world', 'of', 'sosuke', 'and', 'ponyo', 'is', 'a', 'vivid', 'fantasyland', 'intermixed', 'with', 'minimal', 'reality', 'i', 'havent', 'seen', 'animation', 'like', 'this', 'since', 'i', 'was', 'a', 'child', 'and', 'it', 'is', 'wonderful', 'to', 'see', 'it', 'endure', 'and', 'succeedthe', 'actors', 'supplying', 'the', 'voices', 'in', 'the', 'english', 'version', 'were', 'fabulous', 'the', 'length', 'of', 'the', 'movie', 'was', 'perfect', 'especially', 'for', 'children', 'who', 'tend', 'to', 'get', 'squirrelly', 'in', 'films', 'overall', 'a', 'delightful', 'experience', 'worth', 'the', 'very', 'expensive', 'ticket', 'prices', 'we', 'have', 'nowadays'], ['this', 'film', 'was', 'reeeeeeallyyyy', 'bad', 'was', 'it', 'meant', 'to', 'be', 'a', 'comedy', 'as', 'i', 'couldnt', 'help', 'laughing', 'the', 'whole', 'way', 'through', 'it', 'what', 'a', 'waste', 'of', 'two', 'hours', 'donald', 'sutherland', 'was', 'wooden', 'not', 'that', 'he', 'was', 'alone', 'everyone', 'else', 'was', 'just', 'as', 'badand', 'how', 'miscast', 'was', 'linda', 'hamilton'], ['if', 'you', 'have', 'enjoyed', 'the', 'butterfly', 'effect', 'donnie', 'darko', 'or', 'the', 'machinist', 'you', 'will', 'enjoy', 'kpax', 'tooto', 'me', 'this', 'movie', 'felt', 'really', 'uplifting', 'and', 'yet', 'depressing', 'in', 'the', 'end', 'spacey', 'delivers', 'a', 'great', 'performance', 'as', 'prot', 'also', 'lets', 'not', 'forget', 'the', 'appearance', 'of', 'saul', 'williams', 'in', 'the', 'movie', 'who', 'i', 'am', 'a', 'big', 'fan', 'ofafter', 'watching', 'it', 'i', 'recommended', 'the', 'movie', 'to', 'lots', 'of', 'my', 'friends', 'and', 'everyone', 'was', 'pretty', 'much', 'blown', 'awaybut', 'still', 'it', 'is', 'very', 'underrated', 'maybe', 'because', 'of', 'the', 'lack', 'of', 'action', 'and', 'explosions', 'im', 'sorry', 'this', 'is', 'not', 'a', 'movie', 'about', 'blowing', 'things', 'up', 'its', 'about', 'how', 'humans', 'behave', 'and', 'how', 'people', 'live', 'in', 'worlds', 'that', 'dont', 'existgo', 'on', 'and', 'enjoy'], ['the', 'first', 'hour', 'or', 'so', 'of', 'the', 'movie', 'was', 'mostly', 'boring', 'to', 'say', 'the', 'least', 'however', 'it', 'improved', 'afterwards', 'as', 'the', 'valentine', 'party', 'commenced', 'apart', 'from', 'the', 'twist', 'as', 'to', 'the', 'identity', 'of', 'the', 'killer', 'in', 'the', 'very', 'end', 'the', 'hot', 'bath', 'murder', 'scene', 'was', 'one', 'of', 'the', 'few', 'relatively', 'memorable', 'aspects', 'of', 'this', 'movie', 'the', 'scene', 'at', 'the', 'garden', 'with', 'kate', 'was', 'well', 'shot', 'and', 'so', 'was', 'the', 'very', 'last', 'scene', 'the', 'twist', 'in', 'those', 'scenes', 'there', 'was', 'some', 'genuine', 'suspense', 'and', 'thrills', 'and', 'the', 'hot', 'bath', 'murder', 'scene', 'had', 'a', 'nasty', 'the', 'way', 'slashers', 'should', 'be', 'edge', 'to', 'it', 'the', 'earlier', 'murders', 'are', 'frustratingly', 'devoid', 'of', 'gore'], ['as', 'an', 'avid', 'fan', 'of', 'cary', 'grant', 'i', 'expected', 'to', 'watch', 'this', 'movie', 'and', 'howl', 'with', 'laughter', 'as', 'amc', 'billed', 'it', 'as', 'a', 'comedy', 'i', 'have', 'never', 'been', 'more', 'disappointed', 'with', 'a', 'film', 'carys', 'usual', 'charm', 'and', 'effortless', 'comedy', 'are', 'awol', 'from', 'this', 'entire', 'movie', 'he', 'comes', 'across', 'as', 'strained', 'bored', 'and', 'just', 'not', 'himself', 'mississips', 'character', 'ranks', 'among', 'one', 'of', 'the', 'worst', 'stereotypes', 'i', 'have', 'ever', 'witnessed', 'his', 'accent', 'is', 'terribly', 'exaggerated', 'and', 'incorrect', 'according', 'to', 'which', 'part', 'of', 'mississippi', 'he', 'claims', 'to', 'hail', 'from', 'and', 'whenever', 'he', 'does', 'deliver', 'a', 'line', 'its', 'several', 'decibels', 'higher', 'than', 'any', 'other', 'cast', 'member', 'mississip', 'tried', 'to', 'make', 'himself', 'stand', 'out', 'in', 'the', 'film', 'as', 'a', 'lovable', 'countrybumpkin', 'goofball', 'but', 'in', 'the', 'end', 'he', 'manages', 'only', 'to', 'detract', 'from', 'the', 'already', 'weak', 'plot', 'mansfield', 'looks', 'more', 'like', 'an', 'obscene', 'blowup', 'doll', 'than', 'a', 'hollywood', 'sex', 'kitten', 'and', 'while', 'she', 'was', 'never', 'known', 'in', 'hollywood', 'for', 'her', 'acting', 'ability', 'this', 'film', 'screams', 'that', 'she', 'never', 'had', 'that', 'ability', 'to', 'begin', 'with', 'ray', 'walstons', 'character', 'was', 'sugary', 'and', 'ultimately', 'contrived', 'for', 'four', 'men', 'on', 'shore', 'leave', 'it', 'was', 'the', 'tamest', 'leave', 'ive', 'ever', 'seen', 'i', 'watched', 'this', 'nightmare', 'until', 'its', 'very', 'end', 'and', 'while', 'i', 'wont', 'spoil', 'that', 'for', 'anyone', 'i', 'will', 'tell', 'you', 'that', 'its', 'the', 'most', 'absurd', 'youll', 'ever', 'see', 'the', 'film', 'tries', 'to', 'spark', 'patriotism', 'and', 'a', 'sense', 'of', 'debt', 'to', 'the', 'fighting', 'men', 'but', 'the', 'film', 'misses', 'that', 'point', 'totally', 'because', 'of', 'its', 'weak', 'plot', 'line', 'and', 'weak', 'cast', 'sorry', 'cary']]
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\LdTenacity\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[['film', 'entertain', 'lots', 'laughs', 'actors', 'kept', 'film', 'moving', 'along', 'types', 'crazy', 'directions', 'like', 'suggestive', 'language', 'sexy', 'looking', 'gals', 'picture', 'gals', 'guys', 'looking', 'burned', 'even', 'graduate', 'high', 'school', 'one', 'scene', 'teenagers', 'drive', 'car', 'fake', 'deer', 'proceed', 'throw', 'lake', 'ocean', 'repeated', 'horror', 'film', 'except', 'word', 'horrible', 'entire', 'picture', 'arnold', 'plays', 'plastic', 'cop', 'really', 'one', 'sick', 'character', 'please', 'dont', 'waste', 'time', 'viewing', 'film'], ['animation', 'reimagining', 'peter', 'wolf', 'excellent', '29', 'minutes', 'film', 'sleep', 'inducing', 'called', 'peter', 'snails', 'everything', 'moves', 'snails', 'pace', 'couldnt', 'even', 'watch', 'film', 'one', 'sitting', 'watch', '15', 'minutes', 'time', 'pure', 'torturesave', '30', 'minutes', 'watch', 'film', 'thank', 'mei', 'guess', 'oscar', 'nominating', 'committee', 'watched', 'first', 'minutes', 'nominees', 'unfortunately', 'vote', 'winner', 'best', 'animated', 'short', 'short', 'category', 'voters', 'sit', 'whole', 'thing', 'already', 'feel', 'sorry', 'must', 'predict', 'theres', 'way', 'film', 'come', 'close', 'winning'], ['great', 'show', 'make', 'cry', 'group', 'people', 'really', 'loved', 'real', 'life', 'shows', 'time', 'time', 'email', 'lets', 'chat', 'australia', 'real', 'talk', 'like', 'thisi', 'want', 'enjoy', 'five', 'mile', 'creek', 'pass', 'great', 'stories', 'right', 'wrong', 'friendship', 'kids', '40', 'episodes', 'dvdr', 'collected', 'last', '5', 'years', 'see', 'five', 'mile', 'creek', 'tribute', 'wwwmikeandvickicom', 'hear', 'extended', 'theme', 'music', 'lets', 'talk', 'themthese', 'people', 'cool'], ['great', 'tv', 'miniseries', 'classic', 'novel', 'janet', 'mcteer', 'john', 'bowe', 'lead', 'roles', 'exceptional', 'one', 'best', 'adaptations', 'book', 'seen', 'would', 'love', 'get', 'copy', 'let', 'know', 'know', 'might', 'get', 'onethanks'], ['husband', 'dragged', 'film', 'interest', 'seeing', 'anime', 'cartoon', 'absolutely', 'delighted', 'simple', 'story', 'amazing', 'animation', 'digital', 'world', 'effects', 'computer', 'generated', 'refreshing', 'see', 'gorgeous', 'imaginative', 'hand', 'drawn', 'animation', 'world', 'sosuke', 'ponyo', 'vivid', 'fantasyland', 'intermixed', 'minimal', 'reality', 'havent', 'seen', 'animation', 'like', 'since', 'child', 'wonderful', 'see', 'endure', 'succeedthe', 'actors', 'supplying', 'voices', 'english', 'version', 'fabulous', 'length', 'movie', 'perfect', 'especially', 'children', 'tend', 'get', 'squirrelly', 'films', 'overall', 'delightful', 'experience', 'worth', 'expensive', 'ticket', 'prices', 'nowadays'], ['film', 'reeeeeeallyyyy', 'bad', 'meant', 'comedy', 'couldnt', 'help', 'laughing', 'whole', 'way', 'waste', 'two', 'hours', 'donald', 'sutherland', 'wooden', 'alone', 'everyone', 'else', 'badand', 'miscast', 'linda', 'hamilton'], ['enjoyed', 'butterfly', 'effect', 'donnie', 'darko', 'machinist', 'enjoy', 'kpax', 'tooto', 'movie', 'felt', 'really', 'uplifting', 'yet', 'depressing', 'end', 'spacey', 'delivers', 'great', 'performance', 'prot', 'also', 'lets', 'forget', 'appearance', 'saul', 'williams', 'movie', 'big', 'fan', 'ofafter', 'watching', 'recommended', 'movie', 'lots', 'friends', 'everyone', 'pretty', 'much', 'blown', 'awaybut', 'still', 'underrated', 'maybe', 'lack', 'action', 'explosions', 'im', 'sorry', 'movie', 'blowing', 'things', 'humans', 'behave', 'people', 'live', 'worlds', 'dont', 'existgo', 'enjoy'], ['first', 'hour', 'movie', 'mostly', 'boring', 'say', 'least', 'however', 'improved', 'afterwards', 'valentine', 'party', 'commenced', 'apart', 'twist', 'identity', 'killer', 'end', 'hot', 'bath', 'murder', 'scene', 'one', 'relatively', 'memorable', 'aspects', 'movie', 'scene', 'garden', 'kate', 'well', 'shot', 'last', 'scene', 'twist', 'scenes', 'genuine', 'suspense', 'thrills', 'hot', 'bath', 'murder', 'scene', 'nasty', 'way', 'slashers', 'edge', 'earlier', 'murders', 'frustratingly', 'devoid', 'gore'], ['avid', 'fan', 'cary', 'grant', 'expected', 'watch', 'movie', 'howl', 'laughter', 'amc', 'billed', 'comedy', 'never', 'disappointed', 'film', 'carys', 'usual', 'charm', 'effortless', 'comedy', 'awol', 'entire', 'movie', 'comes', 'across', 'strained', 'bored', 'mississips', 'character', 'ranks', 'among', 'one', 'worst', 'stereotypes', 'ever', 'witnessed', 'accent', 'terribly', 'exaggerated', 'incorrect', 'according', 'part', 'mississippi', 'claims', 'hail', 'whenever', 'deliver', 'line', 'several', 'decibels', 'higher', 'cast', 'member', 'mississip', 'tried', 'make', 'stand', 'film', 'lovable', 'countrybumpkin', 'goofball', 'end', 'manages', 'detract', 'already', 'weak', 'plot', 'mansfield', 'looks', 'like', 'obscene', 'blowup', 'doll', 'hollywood', 'sex', 'kitten', 'never', 'known', 'hollywood', 'acting', 'ability', 'film', 'screams', 'never', 'ability', 'begin', 'ray', 'walstons', 'character', 'sugary', 'ultimately', 'contrived', 'four', 'men', 'shore', 'leave', 'tamest', 'leave', 'ive', 'ever', 'seen', 'watched', 'nightmare', 'end', 'wont', 'spoil', 'anyone', 'tell', 'absurd', 'youll', 'ever', 'see', 'film', 'tries', 'spark', 'patriotism', 'sense', 'debt', 'fighting', 'men', 'film', 'misses', 'point', 'totally', 'weak', 'plot', 'line', 'weak', 'cast', 'sorry', 'cary']]
[nltk_data] Downloading package wordnet to
[nltk_data]     C:\Users\LdTenacity\AppData\Roaming\nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     C:\Users\LdTenacity\AppData\Roaming\nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
[['film', 'entertain', 'lot', 'laugh', 'actor', 'kept', 'film', 'moving', 'along', 'type', 'crazy', 'direction', 'like', 'suggestive', 'language', 'sexy', 'looking', 'gal', 'picture', 'gal', 'guy', 'looking', 'burned', 'even', 'graduate', 'high', 'school', 'one', 'scene', 'teenager', 'drive', 'car', 'fake', 'deer', 'proceed', 'throw', 'lake', 'ocean', 'repeated', 'horror', 'film', 'except', 'word', 'horrible', 'entire', 'picture', 'arnold', 'play', 'plastic', 'cop', 'really', 'one', 'sick', 'character', 'please', 'dont', 'waste', 'time', 'viewing', 'film'], ['animation', 'reimagining', 'peter', 'wolf', 'excellent', '29', 'minute', 'film', 'sleep', 'inducing', 'called', 'peter', 'snail', 'everything', 'move', 'snail', 'pace', 'couldnt', 'even', 'watch', 'film', 'one', 'sitting', 'watch', '15', 'minute', 'time', 'pure', 'torturesave', '30', 'minute', 'watch', 'film', 'thank', 'mei', 'guess', 'oscar', 'nominating', 'committee', 'watched', 'first', 'minute', 'nominee', 'unfortunately', 'vote', 'winner', 'best', 'animated', 'short', 'short', 'category', 'voter', 'sit', 'whole', 'thing', 'already', 'feel', 'sorry', 'must', 'predict', 'there', 'way', 'film', 'come', 'close', 'winning'], ['great', 'show', 'make', 'cry', 'group', 'people', 'really', 'loved', 'real', 'life', 'show', 'time', 'time', 'email', 'let', 'chat', 'australia', 'real', 'talk', 'like', 'thisi', 'want', 'enjoy', 'five', 'mile', 'creek', 'pas', 'great', 'story', 'right', 'wrong', 'friendship', 'kid', '40', 'episode', 'dvdr', 'collected', 'last', '5', 'year', 'see', 'five', 'mile', 'creek', 'tribute', 'wwwmikeandvickicom', 'hear', 'extended', 'theme', 'music', 'let', 'talk', 'themthese', 'people', 'cool'], ['great', 'tv', 'miniseries', 'classic', 'novel', 'janet', 'mcteer', 'john', 'bowe', 'lead', 'role', 'exceptional', 'one', 'best', 'adaptation', 'book', 'seen', 'would', 'love', 'get', 'copy', 'let', 'know', 'know', 'might', 'get', 'onethanks'], ['husband', 'dragged', 'film', 'interest', 'seeing', 'anime', 'cartoon', 'absolutely', 'delighted', 'simple', 'story', 'amazing', 'animation', 'digital', 'world', 'effect', 'computer', 'generated', 'refreshing', 'see', 'gorgeous', 'imaginative', 'hand', 'drawn', 'animation', 'world', 'sosuke', 'ponyo', 'vivid', 'fantasyland', 'intermixed', 'minimal', 'reality', 'havent', 'seen', 'animation', 'like', 'since', 'child', 'wonderful', 'see', 'endure', 'succeedthe', 'actor', 'supplying', 'voice', 'english', 'version', 'fabulous', 'length', 'movie', 'perfect', 'especially', 'child', 'tend', 'get', 'squirrelly', 'film', 'overall', 'delightful', 'experience', 'worth', 'expensive', 'ticket', 'price', 'nowadays'], ['film', 'reeeeeeallyyyy', 'bad', 'meant', 'comedy', 'couldnt', 'help', 'laughing', 'whole', 'way', 'waste', 'two', 'hour', 'donald', 'sutherland', 'wooden', 'alone', 'everyone', 'else', 'badand', 'miscast', 'linda', 'hamilton'], ['enjoyed', 'butterfly', 'effect', 'donnie', 'darko', 'machinist', 'enjoy', 'kpax', 'tooto', 'movie', 'felt', 'really', 'uplifting', 'yet', 'depressing', 'end', 'spacey', 'delivers', 'great', 'performance', 'prot', 'also', 'let', 'forget', 'appearance', 'saul', 'williams', 'movie', 'big', 'fan', 'ofafter', 'watching', 'recommended', 'movie', 'lot', 'friend', 'everyone', 'pretty', 'much', 'blown', 'awaybut', 'still', 'underrated', 'maybe', 'lack', 'action', 'explosion', 'im', 'sorry', 'movie', 'blowing', 'thing', 'human', 'behave', 'people', 'live', 'world', 'dont', 'existgo', 'enjoy'], ['first', 'hour', 'movie', 'mostly', 'boring', 'say', 'least', 'however', 'improved', 'afterwards', 'valentine', 'party', 'commenced', 'apart', 'twist', 'identity', 'killer', 'end', 'hot', 'bath', 'murder', 'scene', 'one', 'relatively', 'memorable', 'aspect', 'movie', 'scene', 'garden', 'kate', 'well', 'shot', 'last', 'scene', 'twist', 'scene', 'genuine', 'suspense', 'thrill', 'hot', 'bath', 'murder', 'scene', 'nasty', 'way', 'slasher', 'edge', 'earlier', 'murder', 'frustratingly', 'devoid', 'gore'], ['avid', 'fan', 'cary', 'grant', 'expected', 'watch', 'movie', 'howl', 'laughter', 'amc', 'billed', 'comedy', 'never', 'disappointed', 'film', 'carys', 'usual', 'charm', 'effortless', 'comedy', 'awol', 'entire', 'movie', 'come', 'across', 'strained', 'bored', 'mississips', 'character', 'rank', 'among', 'one', 'worst', 'stereotype', 'ever', 'witnessed', 'accent', 'terribly', 'exaggerated', 'incorrect', 'according', 'part', 'mississippi', 'claim', 'hail', 'whenever', 'deliver', 'line', 'several', 'decibel', 'higher', 'cast', 'member', 'mississip', 'tried', 'make', 'stand', 'film', 'lovable', 'countrybumpkin', 'goofball', 'end', 'manages', 'detract', 'already', 'weak', 'plot', 'mansfield', 'look', 'like', 'obscene', 'blowup', 'doll', 'hollywood', 'sex', 'kitten', 'never', 'known', 'hollywood', 'acting', 'ability', 'film', 'scream', 'never', 'ability', 'begin', 'ray', 'walstons', 'character', 'sugary', 'ultimately', 'contrived', 'four', 'men', 'shore', 'leave', 'tamest', 'leave', 'ive', 'ever', 'seen', 'watched', 'nightmare', 'end', 'wont', 'spoil', 'anyone', 'tell', 'absurd', 'youll', 'ever', 'see', 'film', 'try', 'spark', 'patriotism', 'sense', 'debt', 'fighting', 'men', 'film', 'miss', 'point', 'totally', 'weak', 'plot', 'line', 'weak', 'cast', 'sorry', 'cary']]
Epoch: 1, loss: 0.69330, train_acc: 0.49
Epoch: 2, loss: 14.06390, train_acc: 0.51
Epoch: 3, loss: 4.07358, train_acc: 0.50
Epoch: 4, loss: 2.13637, train_acc: 0.48
Epoch: 5, loss: 0.83345, train_acc: 0.51
Epoch: 6, loss: 0.77699, train_acc: 0.49
Epoch: 7, loss: 2.09756, train_acc: 0.51
Epoch: 8, loss: 1.05628, train_acc: 0.50
Epoch: 9, loss: 2.11393, train_acc: 0.49
Epoch: 10, loss: 1.39586, train_acc: 0.49
Epoch: 11, loss: 1.54989, train_acc: 0.51
Epoch: 12, loss: 1.74546, train_acc: 0.51
Epoch: 13, loss: 0.84594, train_acc: 0.49
Epoch: 14, loss: 1.71859, train_acc: 0.49
Epoch: 15, loss: 0.98677, train_acc: 0.49
Epoch: 16, loss: 1.43415, train_acc: 0.51
Epoch: 17, loss: 1.45399, train_acc: 0.51
Epoch: 18, loss: 1.91517, train_acc: 0.50
Epoch: 19, loss: 1.61463, train_acc: 0.49
Epoch: 20, loss: 1.13616, train_acc: 0.50
Epoch: 21, loss: 1.64171, train_acc: 0.49
Epoch: 22, loss: 1.68830, train_acc: 0.50
Epoch: 23, loss: 1.16542, train_acc: 0.51
Epoch: 24, loss: 0.93974, train_acc: 0.50
Epoch: 25, loss: 1.43778, train_acc: 0.50
Finished Training
              precision    recall  f1-score   support

           0       0.52      0.73      0.61       132
           1       0.44      0.24      0.31       118

    accuracy                           0.50       250
   macro avg       0.48      0.49      0.46       250
weighted avg       0.48      0.50      0.47       250

input_torch torch.Size([750, 100])
sparsemans loss -0.46959036161969436
Epoch: 1, loss: 0.22351, train_acc: 0.50
input_torch torch.Size([750, 100])
sparsemans loss 0.3212940237151188
Epoch: 2, loss: 1.01994, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.49556080171435113
Epoch: 3, loss: 1.18788, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.3460018860538457
Epoch: 4, loss: 1.03859, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.200413878049827
Epoch: 5, loss: 0.89399, train_acc: 0.50
input_torch torch.Size([750, 100])
sparsemans loss 0.1733811806831236
Epoch: 6, loss: 0.86554, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.10328490635986202
Epoch: 7, loss: 0.79349, train_acc: 0.56
input_torch torch.Size([750, 100])
sparsemans loss -0.05410479280953089
Epoch: 8, loss: 0.63488, train_acc: 0.55
input_torch torch.Size([750, 100])
sparsemans loss -0.13352670838264769
Epoch: 9, loss: 0.55496, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.08920187340133486
Epoch: 10, loss: 0.77714, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.5219529867222854
Epoch: 11, loss: 1.20859, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.44055029239339094
Epoch: 12, loss: 1.12514, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.36677936283446716
Epoch: 13, loss: 1.04898, train_acc: 0.62
input_torch torch.Size([750, 100])
sparsemans loss 0.4383080370085582
Epoch: 14, loss: 1.11799, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss 0.5095875870883378
Epoch: 15, loss: 1.18636, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss 0.5680495359995139
Epoch: 16, loss: 1.24084, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss 0.1952321088077991
Epoch: 17, loss: 0.86235, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss 0.004354756078346252
Epoch: 18, loss: 0.66452, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss -0.04473349908667771
Epoch: 19, loss: 0.60718, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss -0.13659611235985952
Epoch: 20, loss: 0.50202, train_acc: 0.73
input_torch torch.Size([750, 100])
sparsemans loss -0.11674092773499407
Epoch: 21, loss: 0.50623, train_acc: 0.79
input_torch torch.Size([750, 100])
sparsemans loss 0.09571592977152277
Epoch: 22, loss: 0.69246, train_acc: 0.87
input_torch torch.Size([750, 100])
sparsemans loss 0.37069857818699997
Epoch: 23, loss: 0.93929, train_acc: 0.80
input_torch torch.Size([750, 100])
sparsemans loss 0.5760226779635269
Epoch: 24, loss: 1.15225, train_acc: 0.68
input_torch torch.Size([750, 100])
sparsemans loss 0.5518576264927864
Epoch: 25, loss: 1.06238, train_acc: 0.76
Finished Training
              precision    recall  f1-score   support

           0       0.63      0.39      0.49       132
           1       0.52      0.75      0.62       118

    accuracy                           0.56       250
   macro avg       0.58      0.57      0.55       250
weighted avg       0.58      0.56      0.55       250

input_torch torch.Size([750, 100])
sparsemans loss -0.46959036161969436
Epoch: 1, loss: 0.22361, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss 0.7890943019072149
Epoch: 2, loss: 1.48663, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.7100078462728544
Epoch: 3, loss: 1.40219, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.767635283362443
Epoch: 4, loss: 1.46054, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss 0.8547855294520905
Epoch: 5, loss: 1.54818, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss 0.870468489157481
Epoch: 6, loss: 1.56231, train_acc: 0.52
input_torch torch.Size([750, 100])
sparsemans loss 0.8728249835029789
Epoch: 7, loss: 1.56293, train_acc: 0.55
input_torch torch.Size([750, 100])
sparsemans loss 0.8898315693607798
Epoch: 8, loss: 1.57895, train_acc: 0.58
input_torch torch.Size([750, 100])
sparsemans loss 0.8928392400053697
Epoch: 9, loss: 1.58150, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.8703959185455614
Epoch: 10, loss: 1.55834, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.9529146157128909
Epoch: 11, loss: 1.63938, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 1.0083530273253483
Epoch: 12, loss: 1.69271, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.9227882869185112
Epoch: 13, loss: 1.60478, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss 0.9293016644276719
Epoch: 14, loss: 1.60873, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss 0.9248564292370195
Epoch: 15, loss: 1.60114, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss 0.8793322474395363
Epoch: 16, loss: 1.55120, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss 0.8540709008359875
Epoch: 17, loss: 1.51981, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss 0.9105613848984763
Epoch: 18, loss: 1.56892, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss 0.8290309814071167
Epoch: 19, loss: 1.47711, train_acc: 0.71
input_torch torch.Size([750, 100])
sparsemans loss 1.3676202898561334
Epoch: 20, loss: 2.00017, train_acc: 0.77
input_torch torch.Size([750, 100])
sparsemans loss 1.170995595385659
Epoch: 21, loss: 1.78220, train_acc: 0.85
input_torch torch.Size([750, 100])
sparsemans loss 1.0753372483809231
Epoch: 22, loss: 1.65155, train_acc: 0.88
input_torch torch.Size([750, 100])
sparsemans loss 0.9024088128367397
Epoch: 23, loss: 1.39896, train_acc: 0.89
input_torch torch.Size([750, 100])
sparsemans loss 1.0652693789855785
Epoch: 24, loss: 1.71802, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss 0.5674268179654707
Epoch: 25, loss: 1.27252, train_acc: 0.62
Finished Training
              precision    recall  f1-score   support

           0       0.51      0.63      0.56       132
           1       0.44      0.33      0.38       118

    accuracy                           0.49       250
   macro avg       0.48      0.48      0.47       250
weighted avg       0.48      0.49      0.48       250

weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.67309, train_acc: 0.51
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.67566, train_acc: 0.49
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.67187, train_acc: 0.52
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.67317, train_acc: 0.51
weight adjusted tensor(0.0207, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.67135, train_acc: 0.51
weight adjusted tensor(0.0207, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.66962, train_acc: 0.51
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.66962, train_acc: 0.57
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.66928, train_acc: 0.55
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.66840, train_acc: 0.55
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.66668, train_acc: 0.57
weight adjusted tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.66452, train_acc: 0.61
weight adjusted tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.66226, train_acc: 0.63
weight adjusted tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.65982, train_acc: 0.60
weight adjusted tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.65646, train_acc: 0.64
weight adjusted tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.65142, train_acc: 0.68
weight adjusted tensor(0.0206, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.64396, train_acc: 0.67
weight adjusted tensor(0.0206, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.63583, train_acc: 0.67
weight adjusted tensor(0.0206, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.62380, train_acc: 0.69
weight adjusted tensor(0.0206, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.60513, train_acc: 0.73
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.57853, train_acc: 0.83
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.53976, train_acc: 0.78
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.67881, train_acc: 0.52
weight adjusted tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.51976, train_acc: 0.83
weight adjusted tensor(0.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.52550, train_acc: 0.79
weight adjusted tensor(0.0199, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 0.54655, train_acc: 0.67
Finished Training
              precision    recall  f1-score   support

           0       0.55      0.90      0.68       132
           1       0.59      0.16      0.25       118

    accuracy                           0.55       250
   macro avg       0.57      0.53      0.47       250
weighted avg       0.57      0.55      0.48       250

sparsemans loss -0.046959036161969434
word analogy weight adjusted tensor(1.9976, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: -1.35105, train_acc: 0.49
sparsemans loss -0.018690915494806712
word analogy weight adjusted tensor(1.9989, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: -1.32015, train_acc: 0.51
sparsemans loss 0.05421673368800819
word analogy weight adjusted tensor(2.0775, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: -1.33071, train_acc: 0.51
sparsemans loss 0.061993139659591454
word analogy weight adjusted tensor(2.1564, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: -1.40228, train_acc: 0.51
sparsemans loss 0.05962717613854077
word analogy weight adjusted tensor(2.3135, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: -1.56087, train_acc: 0.50
sparsemans loss 0.038417858753977824
word analogy weight adjusted tensor(2.3140, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: -1.58359, train_acc: 0.51
sparsemans loss 0.028021227821925235
word analogy weight adjusted tensor(2.0803, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: -1.36193, train_acc: 0.56
sparsemans loss 0.023456407795678622
word analogy weight adjusted tensor(2.0014, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: -1.28896, train_acc: 0.57
sparsemans loss 0.04015888467905296
word analogy weight adjusted tensor(2.0025, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: -1.27404, train_acc: 0.51
sparsemans loss 0.05070575025544275
word analogy weight adjusted tensor(2.0002, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: -1.26203, train_acc: 0.51
sparsemans loss 0.052677851712539255
word analogy weight adjusted tensor(2.0014, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: -1.26271, train_acc: 0.51
sparsemans loss 0.04434833569280753
word analogy weight adjusted tensor(2.0016, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: -1.27346, train_acc: 0.51
sparsemans loss 0.04608413774976704
word analogy weight adjusted tensor(2.0030, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: -1.27589, train_acc: 0.65
sparsemans loss 0.05158906768177521
word analogy weight adjusted tensor(2.0017, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: -1.27208, train_acc: 0.65
sparsemans loss 0.029728087619101166
word analogy weight adjusted tensor(1.9972, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: -1.29316, train_acc: 0.66
sparsemans loss 0.021382430516729837
word analogy weight adjusted tensor(1.9919, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: -1.30139, train_acc: 0.66
sparsemans loss 0.012549101715975987
word analogy weight adjusted tensor(1.9943, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: -1.31989, train_acc: 0.67
sparsemans loss 0.0054824727566510105
word analogy weight adjusted tensor(1.9988, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: -1.34054, train_acc: 0.70
sparsemans loss 0.003526451273708628
word analogy weight adjusted tensor(1.9927, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: -1.35015, train_acc: 0.77
sparsemans loss 0.02930564917977396
word analogy weight adjusted tensor(1.9796, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: -1.33227, train_acc: 0.85
sparsemans loss 0.036536450808985155
word analogy weight adjusted tensor(1.9418, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: -1.32064, train_acc: 0.90
sparsemans loss -0.006608792078990869
word analogy weight adjusted tensor(1.9083, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: -1.35828, train_acc: 0.71
sparsemans loss -0.04519879918606352
word analogy weight adjusted tensor(1.8393, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: -1.16779, train_acc: 0.52
sparsemans loss -0.09877800311835377
word analogy weight adjusted tensor(1.8369, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: -1.26656, train_acc: 0.54
sparsemans loss -0.09581775686723576
word analogy weight adjusted tensor(1.8318, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: -1.33162, train_acc: 0.65
Finished Training
              precision    recall  f1-score   support

           0       0.51      0.67      0.58       132
           1       0.44      0.29      0.35       118

    accuracy                           0.49       250
   macro avg       0.47      0.48      0.46       250
weighted avg       0.48      0.49      0.47       250

Epoch: 1, loss: 0.69339, train_acc: 0.49
Epoch: 2, loss: 14.22793, train_acc: 0.51
Epoch: 3, loss: 3.97862, train_acc: 0.48
Epoch: 4, loss: 2.61662, train_acc: 0.49
Epoch: 5, loss: 0.91674, train_acc: 0.53
Epoch: 6, loss: 1.14395, train_acc: 0.51
Epoch: 7, loss: 0.96856, train_acc: 0.50
Epoch: 8, loss: 1.04735, train_acc: 0.50
Epoch: 9, loss: 1.28131, train_acc: 0.48
Epoch: 10, loss: 0.95993, train_acc: 0.47
Epoch: 11, loss: 1.10691, train_acc: 0.50
Epoch: 12, loss: 1.21047, train_acc: 0.52
Epoch: 13, loss: 0.95023, train_acc: 0.51
Epoch: 14, loss: 1.13140, train_acc: 0.50
Epoch: 15, loss: 1.04469, train_acc: 0.51
Epoch: 16, loss: 1.09836, train_acc: 0.50
Epoch: 17, loss: 1.49910, train_acc: 0.47
Epoch: 18, loss: 1.35727, train_acc: 0.48
Epoch: 19, loss: 1.16374, train_acc: 0.48
Epoch: 20, loss: 1.06480, train_acc: 0.49
Epoch: 21, loss: 0.92790, train_acc: 0.49
Epoch: 22, loss: 1.06016, train_acc: 0.49
Epoch: 23, loss: 0.95541, train_acc: 0.52
Epoch: 24, loss: 0.85451, train_acc: 0.48
Epoch: 25, loss: 0.90931, train_acc: 0.49
Finished Training
              precision    recall  f1-score   support

           0       0.59      0.66      0.62       132
           1       0.56      0.48      0.52       118

    accuracy                           0.58       250
   macro avg       0.57      0.57      0.57       250
weighted avg       0.57      0.58      0.57       250

test_pad.shape: 250
test_pad[0].shape: 100
weight adjusted tensor(3.1133, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.69357, train_acc: 0.49
weight adjusted tensor(3.1137, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.69308, train_acc: 0.51
weight adjusted tensor(3.1148, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.69145, train_acc: 0.51
weight adjusted tensor(3.1155, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.69021, train_acc: 0.58
weight adjusted tensor(3.1162, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.68905, train_acc: 0.59
weight adjusted tensor(3.1167, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.68711, train_acc: 0.61
weight adjusted tensor(3.1191, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.68435, train_acc: 0.59
weight adjusted tensor(3.1111, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.68092, train_acc: 0.59
weight adjusted tensor(3.1096, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.67638, train_acc: 0.60
weight adjusted tensor(3.1095, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.66955, train_acc: 0.62
weight adjusted tensor(3.1063, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.65953, train_acc: 0.65
weight adjusted tensor(3.1033, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.64485, train_acc: 0.68
weight adjusted tensor(3.1005, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.61749, train_acc: 0.70
weight adjusted tensor(3.0981, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.59453, train_acc: 0.68
weight adjusted tensor(3.0948, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.72805, train_acc: 0.67
weight adjusted tensor(3.0927, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.55887, train_acc: 0.68
weight adjusted tensor(3.0905, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.57858, train_acc: 0.64
weight adjusted tensor(3.0890, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.57678, train_acc: 0.66
weight adjusted tensor(3.0838, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.57532, train_acc: 0.71
weight adjusted tensor(3.0830, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.57236, train_acc: 0.75
weight adjusted tensor(3.0821, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.56700, train_acc: 0.80
weight adjusted tensor(3.0797, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.55757, train_acc: 0.83
weight adjusted tensor(3.0470, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.53912, train_acc: 0.84
weight adjusted tensor(3.0468, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.47796, train_acc: 0.84
weight adjusted tensor(3.1252, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 1.48278, train_acc: 0.63
Finished Training
              precision    recall  f1-score   support

           0       0.63      0.74      0.68       132
           1       0.64      0.52      0.57       118

    accuracy                           0.64       250
   macro avg       0.64      0.63      0.63       250
weighted avg       0.64      0.64      0.63       250

input_torch torch.Size([750, 100])
sparsemans loss -1.0612900046106426
Epoch: 1, loss: -0.36808, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -0.47032306094517545
Epoch: 2, loss: 0.22254, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss 0.33429637684494995
Epoch: 3, loss: 1.02517, train_acc: 0.54
input_torch torch.Size([750, 100])
sparsemans loss 0.2409518263868971
Epoch: 4, loss: 0.93086, train_acc: 0.52
input_torch torch.Size([750, 100])
sparsemans loss 0.11149041332199564
Epoch: 5, loss: 0.79997, train_acc: 0.52
input_torch torch.Size([750, 100])
sparsemans loss 0.12977438100710972
Epoch: 6, loss: 0.81589, train_acc: 0.56
input_torch torch.Size([750, 100])
sparsemans loss 0.14690830449165027
Epoch: 7, loss: 0.83027, train_acc: 0.60
input_torch torch.Size([750, 100])
sparsemans loss 0.28245450310694264
Epoch: 8, loss: 0.96239, train_acc: 0.62
input_torch torch.Size([750, 100])
sparsemans loss 0.8095389421998022
Epoch: 9, loss: 1.48419, train_acc: 0.64
input_torch torch.Size([750, 100])
sparsemans loss 0.9851069144424657
Epoch: 10, loss: 1.65171, train_acc: 0.64
input_torch torch.Size([750, 100])
sparsemans loss 1.1264185839533407
Epoch: 11, loss: 1.78163, train_acc: 0.63
input_torch torch.Size([750, 100])
sparsemans loss 0.5859355169757077
Epoch: 12, loss: 1.22191, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss 0.524417813314267
Epoch: 13, loss: 1.16928, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss 0.5758411160417164
Epoch: 14, loss: 1.29180, train_acc: 0.62
input_torch torch.Size([750, 100])
sparsemans loss 0.6610810321907806
Epoch: 15, loss: 1.26967, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss 1.1913125502000497
Epoch: 16, loss: 1.79293, train_acc: 0.71
input_torch torch.Size([750, 100])
sparsemans loss 1.171724547560891
Epoch: 17, loss: 1.76667, train_acc: 0.75
input_torch torch.Size([750, 100])
sparsemans loss 1.3311027824162678
Epoch: 18, loss: 1.91423, train_acc: 0.76
input_torch torch.Size([750, 100])
sparsemans loss 1.3237850153456707
Epoch: 19, loss: 1.87946, train_acc: 0.77
input_torch torch.Size([750, 100])
sparsemans loss 1.3309761324395628
Epoch: 20, loss: 1.88104, train_acc: 0.72
input_torch torch.Size([750, 100])
sparsemans loss 1.288420718800602
Epoch: 21, loss: 1.83335, train_acc: 0.79
input_torch torch.Size([750, 100])
sparsemans loss 1.2989878104205035
Epoch: 22, loss: 1.87711, train_acc: 0.71
input_torch torch.Size([750, 100])
sparsemans loss 2.06370399889236
Epoch: 23, loss: 2.65304, train_acc: 0.68
input_torch torch.Size([750, 100])
sparsemans loss 2.141550267994046
Epoch: 24, loss: 2.72966, train_acc: 0.68
input_torch torch.Size([750, 100])
sparsemans loss 2.2079056881896233
Epoch: 25, loss: 2.78797, train_acc: 0.69
Finished Training
              precision    recall  f1-score   support

           0       0.58      0.93      0.72       132
           1       0.76      0.25      0.37       118

    accuracy                           0.61       250
   macro avg       0.67      0.59      0.54       250
weighted avg       0.67      0.61      0.55       250

weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.66285, train_acc: 0.49
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.66225, train_acc: 0.51
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.66079, train_acc: 0.51
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.65956, train_acc: 0.59
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.65857, train_acc: 0.58
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.65708, train_acc: 0.58
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.65481, train_acc: 0.60
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.65206, train_acc: 0.60
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.64860, train_acc: 0.58
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.64353, train_acc: 0.60
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.63603, train_acc: 0.63
weight adjusted tensor(0.0311, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.62549, train_acc: 0.66
weight adjusted tensor(0.0310, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.60900, train_acc: 0.68
weight adjusted tensor(0.0302, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.58302, train_acc: 0.66
weight adjusted tensor(0.0301, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.63133, train_acc: 0.67
weight adjusted tensor(0.0301, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.55149, train_acc: 0.65
weight adjusted tensor(0.0316, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.57035, train_acc: 0.61
weight adjusted tensor(0.0308, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.55469, train_acc: 0.66
weight adjusted tensor(0.0315, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.54662, train_acc: 0.75
weight adjusted tensor(0.0320, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.54224, train_acc: 0.81
weight adjusted tensor(0.0316, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.53706, train_acc: 0.82
weight adjusted tensor(0.0316, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.52543, train_acc: 0.83
weight adjusted tensor(0.0316, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.50009, train_acc: 0.85
weight adjusted tensor(0.0316, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.42773, train_acc: 0.84
weight adjusted tensor(0.0312, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 1.52034, train_acc: 0.62
Finished Training
              precision    recall  f1-score   support

           0       0.65      0.75      0.70       132
           1       0.66      0.55      0.60       118

    accuracy                           0.66       250
   macro avg       0.66      0.65      0.65       250
weighted avg       0.66      0.66      0.65       250

sparsemans loss -0.10612900046106426
word analogy weight adjusted tensor(3.1133, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: -2.52623, train_acc: 0.49
sparsemans loss 0.20450095004560787
word analogy weight adjusted tensor(3.1133, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: -2.21649, train_acc: 0.51
sparsemans loss 0.05460698203099686
word analogy weight adjusted tensor(3.1115, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: -2.36616, train_acc: 0.52
sparsemans loss 0.04370769750181738
word analogy weight adjusted tensor(3.1894, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: -2.45622, train_acc: 0.59
sparsemans loss 0.16132980040229034
word analogy weight adjusted tensor(3.1134, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: -2.26435, train_acc: 0.60
sparsemans loss 0.1513822073174908
word analogy weight adjusted tensor(3.1078, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: -2.27137, train_acc: 0.59
sparsemans loss 0.17048920276003302
word analogy weight adjusted tensor(3.1083, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: -2.25631, train_acc: 0.59
sparsemans loss 0.22838275430312266
word analogy weight adjusted tensor(3.1078, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: -2.20253, train_acc: 0.60
sparsemans loss 0.16887387616659397
word analogy weight adjusted tensor(3.1070, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: -2.26845, train_acc: 0.61
sparsemans loss 0.1913209739672132
word analogy weight adjusted tensor(3.1046, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: -2.25446, train_acc: 0.65
sparsemans loss 0.17315655380674258
word analogy weight adjusted tensor(3.0993, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: -2.28433, train_acc: 0.67
sparsemans loss 0.11838256677355116
word analogy weight adjusted tensor(3.0979, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: -2.36990, train_acc: 0.69
sparsemans loss 0.11025100477670324
word analogy weight adjusted tensor(3.0959, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: -2.07247, train_acc: 0.58
sparsemans loss 0.15942813864187197
word analogy weight adjusted tensor(3.0884, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: -2.12672, train_acc: 0.67
sparsemans loss 0.15713084128227423
word analogy weight adjusted tensor(3.0837, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: -2.32849, train_acc: 0.73
sparsemans loss 0.17813570120919245
word analogy weight adjusted tensor(3.0798, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: -2.31511, train_acc: 0.74
sparsemans loss 0.18252218802867162
word analogy weight adjusted tensor(3.0457, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: -2.26435, train_acc: 0.74
sparsemans loss 0.23140916433093087
word analogy weight adjusted tensor(3.0412, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: -2.20390, train_acc: 0.74
sparsemans loss 0.2319479936795654
word analogy weight adjusted tensor(3.0392, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: -2.19765, train_acc: 0.74
sparsemans loss 0.22680530180130096
word analogy weight adjusted tensor(3.0358, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: -2.19773, train_acc: 0.74
sparsemans loss 0.22514423222072638
word analogy weight adjusted tensor(3.0322, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: -2.19560, train_acc: 0.73
sparsemans loss 0.22662276303702086
word analogy weight adjusted tensor(3.0301, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: -2.19303, train_acc: 0.73
sparsemans loss 0.2322824829812632
word analogy weight adjusted tensor(3.0257, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: -2.18481, train_acc: 0.74
sparsemans loss 0.24128077726623084
word analogy weight adjusted tensor(2.9936, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: -2.14643, train_acc: 0.74
sparsemans loss 0.2487915724886325
word analogy weight adjusted tensor(2.9928, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: -2.14171, train_acc: 0.74
Finished Training
              precision    recall  f1-score   support

           0       0.71      0.23      0.34       132
           1       0.51      0.90      0.65       118

    accuracy                           0.54       250
   macro avg       0.61      0.56      0.50       250
weighted avg       0.62      0.54      0.49       250

test_pad.shape: 250
test_pad[0].shape: 100
weight adjusted tensor(54.9540, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.69310, train_acc: 0.51
weight adjusted tensor(54.9593, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.69545, train_acc: 0.51
weight adjusted tensor(54.9467, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.69554, train_acc: 0.52
weight adjusted tensor(54.9374, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.69359, train_acc: 0.55
weight adjusted tensor(54.9301, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.69141, train_acc: 0.51
weight adjusted tensor(54.9241, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.69132, train_acc: 0.54
weight adjusted tensor(54.9182, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.69128, train_acc: 0.51
weight adjusted tensor(54.9129, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.69031, train_acc: 0.56
weight adjusted tensor(54.9085, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.68919, train_acc: 0.52
weight adjusted tensor(54.9055, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.68841, train_acc: 0.51
weight adjusted tensor(54.9038, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.68752, train_acc: 0.51
weight adjusted tensor(54.9031, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.68592, train_acc: 0.51
weight adjusted tensor(54.9033, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.68363, train_acc: 0.51
weight adjusted tensor(54.9040, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.68102, train_acc: 0.73
weight adjusted tensor(54.9051, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.67811, train_acc: 0.69
weight adjusted tensor(54.9064, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.67407, train_acc: 0.68
weight adjusted tensor(54.9071, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.66813, train_acc: 0.63
weight adjusted tensor(54.9082, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.66059, train_acc: 0.71
weight adjusted tensor(54.9101, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.65006, train_acc: 0.74
weight adjusted tensor(54.9119, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.63319, train_acc: 0.75
weight adjusted tensor(54.9129, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.60774, train_acc: 0.81
weight adjusted tensor(54.9077, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.60761, train_acc: 0.52
weight adjusted tensor(54.8997, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.66573, train_acc: 0.53
weight adjusted tensor(54.8882, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.54883, train_acc: 0.91
weight adjusted tensor(54.8769, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 0.56068, train_acc: 0.74
Finished Training
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       132
           1       0.47      1.00      0.64       118

    accuracy                           0.47       250
   macro avg       0.24      0.50      0.32       250
weighted avg       0.22      0.47      0.30       250

input_torch torch.Size([750, 100])
sparsemans loss -1.4386842997117986
Epoch: 1, loss: -0.74428, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss -1.4424170843651662
Epoch: 2, loss: -0.74413, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -1.4431479285838933
Epoch: 3, loss: -0.74987, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -1.4393798627513366
Epoch: 4, loss: -0.74736, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -1.539022607520651
Epoch: 5, loss: -0.84591, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss -1.5353424375602323
Epoch: 6, loss: -0.84254, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss -1.534497410852315
Epoch: 7, loss: -0.84313, train_acc: 0.54
input_torch torch.Size([750, 100])
sparsemans loss -1.5262973316846469
Epoch: 8, loss: -0.83620, train_acc: 0.52
input_torch torch.Size([750, 100])
sparsemans loss -1.525211545795974
Epoch: 9, loss: -0.83578, train_acc: 0.54
input_torch torch.Size([750, 100])
sparsemans loss -1.532310984708411
Epoch: 10, loss: -0.84327, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -1.5325790790042617
Epoch: 11, loss: -0.84428, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -1.5370021533296834
Epoch: 12, loss: -0.85011, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -1.5318039166931259
Epoch: 13, loss: -0.84682, train_acc: 0.54
input_torch torch.Size([750, 100])
sparsemans loss -1.5291811902345516
Epoch: 14, loss: -0.84630, train_acc: 0.69
input_torch torch.Size([750, 100])
sparsemans loss -1.5282169109458181
Epoch: 15, loss: -0.84757, train_acc: 0.56
input_torch torch.Size([750, 100])
sparsemans loss -1.5225991892925403
Epoch: 16, loss: -0.84474, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss -1.5232302304173129
Epoch: 17, loss: -0.84941, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss -1.5264971901894404
Epoch: 18, loss: -0.85837, train_acc: 0.59
input_torch torch.Size([750, 100])
sparsemans loss -1.5276431729161086
Epoch: 19, loss: -0.86644, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss -1.5271549370135695
Epoch: 20, loss: -0.87496, train_acc: 0.69
input_torch torch.Size([750, 100])
sparsemans loss -1.5169863291353602
Epoch: 21, loss: -0.87931, train_acc: 0.73
input_torch torch.Size([750, 100])
sparsemans loss -1.5045672854907348
Epoch: 22, loss: -0.88488, train_acc: 0.79
input_torch torch.Size([750, 100])
sparsemans loss -1.5164269010595053
Epoch: 23, loss: -0.92862, train_acc: 0.84
input_torch torch.Size([750, 100])
sparsemans loss -1.5131980057572187
Epoch: 24, loss: -0.97988, train_acc: 0.83
input_torch torch.Size([750, 100])
sparsemans loss -1.5096960623063147
Epoch: 25, loss: -0.34765, train_acc: 0.51
Finished Training
              precision    recall  f1-score   support

           0       0.54      0.99      0.70       132
           1       0.83      0.04      0.08       118

    accuracy                           0.54       250
   macro avg       0.69      0.52      0.39       250
weighted avg       0.68      0.54      0.41       250

weight adjusted tensor(0.5495, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.14521, train_acc: 0.49
weight adjusted tensor(0.5495, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.14902, train_acc: 0.51
weight adjusted tensor(0.5495, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.14400, train_acc: 0.51
weight adjusted tensor(0.5495, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.14241, train_acc: 0.52
weight adjusted tensor(0.5495, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.14349, train_acc: 0.49
weight adjusted tensor(0.5495, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.14330, train_acc: 0.49
weight adjusted tensor(0.5494, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.14188, train_acc: 0.57
weight adjusted tensor(0.5493, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.14067, train_acc: 0.51
weight adjusted tensor(0.5493, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.14019, train_acc: 0.69
weight adjusted tensor(0.5493, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.13993, train_acc: 0.51
weight adjusted tensor(0.5492, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.13917, train_acc: 0.51
weight adjusted tensor(0.5492, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.13772, train_acc: 0.51
weight adjusted tensor(0.5492, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.13592, train_acc: 0.71
weight adjusted tensor(0.5491, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.13407, train_acc: 0.61
weight adjusted tensor(0.5491, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.13211, train_acc: 0.61
weight adjusted tensor(0.5491, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.12957, train_acc: 0.69
weight adjusted tensor(0.5491, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.12585, train_acc: 0.69
weight adjusted tensor(0.5490, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.12066, train_acc: 0.64
weight adjusted tensor(0.5490, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.11429, train_acc: 0.67
weight adjusted tensor(0.5489, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.10641, train_acc: 0.70
weight adjusted tensor(0.5489, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.09333, train_acc: 0.67
weight adjusted tensor(0.5488, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.07751, train_acc: 0.74
weight adjusted tensor(0.5486, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.04912, train_acc: 0.74
weight adjusted tensor(0.5485, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.06904, train_acc: 0.71
weight adjusted tensor(0.5483, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 0.51521, train_acc: 0.49
Finished Training
              precision    recall  f1-score   support

           0       0.53      1.00      0.69       132
           1       1.00      0.02      0.03       118

    accuracy                           0.54       250
   macro avg       0.77      0.51      0.36       250
weighted avg       0.75      0.54      0.38       250

sparsemans loss -0.14386842997117985
word analogy weight adjusted tensor(54.9540, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: -54.40470, train_acc: 0.49
sparsemans loss -0.14460705273329144
word analogy weight adjusted tensor(54.9471, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: -54.39040, train_acc: 0.51
sparsemans loss -0.1440159486701176
word analogy weight adjusted tensor(54.9462, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: -54.39747, train_acc: 0.51
sparsemans loss -0.14358987562398542
word analogy weight adjusted tensor(54.9465, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: -54.39737, train_acc: 0.49
sparsemans loss -0.14384518986048145
word analogy weight adjusted tensor(54.9414, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: -54.39071, train_acc: 0.49
sparsemans loss -0.14386017348385208
word analogy weight adjusted tensor(54.9320, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: -54.38270, train_acc: 0.49
sparsemans loss -0.14406511861423651
word analogy weight adjusted tensor(54.9212, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: -54.37427, train_acc: 0.55
sparsemans loss -0.15362113869677715
word analogy weight adjusted tensor(54.9109, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: -54.37468, train_acc: 0.54
sparsemans loss -0.1536692475908884
word analogy weight adjusted tensor(54.9019, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: -54.36600, train_acc: 0.51
sparsemans loss -0.1527959196692959
word analogy weight adjusted tensor(54.8944, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: -54.35777, train_acc: 0.51
sparsemans loss -0.1529408418501237
word analogy weight adjusted tensor(54.8890, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: -54.35323, train_acc: 0.51
sparsemans loss -0.15408158755876877
word analogy weight adjusted tensor(54.8841, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: -54.35093, train_acc: 0.51
sparsemans loss -0.1530340710931823
word analogy weight adjusted tensor(54.8796, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: -54.34724, train_acc: 0.53
sparsemans loss -0.15186020209651213
word analogy weight adjusted tensor(54.8756, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: -54.34385, train_acc: 0.64
sparsemans loss -0.15178992015288367
word analogy weight adjusted tensor(54.8720, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: -54.34194, train_acc: 0.57
sparsemans loss -0.1517410436058494
word analogy weight adjusted tensor(54.8674, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: -54.33945, train_acc: 0.68
sparsemans loss -0.15259208373927277
word analogy weight adjusted tensor(54.8621, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: -54.33787, train_acc: 0.70
sparsemans loss -0.15235574042451816
word analogy weight adjusted tensor(54.8557, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: -54.33550, train_acc: 0.69
sparsemans loss -0.1511018111332831
word analogy weight adjusted tensor(54.8486, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: -54.33280, train_acc: 0.65
sparsemans loss -0.15021257008539687
word analogy weight adjusted tensor(54.8410, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: -54.33126, train_acc: 0.69
sparsemans loss -0.1492319848868095
word analogy weight adjusted tensor(54.8322, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: -54.33054, train_acc: 0.74
sparsemans loss -0.14965223199177982
word analogy weight adjusted tensor(54.8237, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: -54.33685, train_acc: 0.75
sparsemans loss -0.1486807511424308
word analogy weight adjusted tensor(54.8143, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: -54.34770, train_acc: 0.82
sparsemans loss -0.14818475350620436
word analogy weight adjusted tensor(54.8024, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: -54.36778, train_acc: 0.81
sparsemans loss -0.14760595064813578
word analogy weight adjusted tensor(54.7864, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: -54.36598, train_acc: 0.67
Finished Training
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.53      1.00      0.69       132
           1       0.00      0.00      0.00       118

    accuracy                           0.53       250
   macro avg       0.26      0.50      0.35       250
weighted avg       0.28      0.53      0.36       250

test_pad.shape: 250
test_pad[0].shape: 100
weight adjusted tensor(23.4717, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.69330, train_acc: 0.51
weight adjusted tensor(23.4712, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.69381, train_acc: 0.51
weight adjusted tensor(23.4737, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.69129, train_acc: 0.55
weight adjusted tensor(23.4744, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.69131, train_acc: 0.56
weight adjusted tensor(23.4722, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.69028, train_acc: 0.56
weight adjusted tensor(23.4688, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.68841, train_acc: 0.51
weight adjusted tensor(23.4657, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.68688, train_acc: 0.58
weight adjusted tensor(23.4638, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.68525, train_acc: 0.58
weight adjusted tensor(23.4631, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.68239, train_acc: 0.59
weight adjusted tensor(23.4630, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.67816, train_acc: 0.62
weight adjusted tensor(23.4642, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.67289, train_acc: 0.62
weight adjusted tensor(23.4625, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.66535, train_acc: 0.65
weight adjusted tensor(23.4583, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.65207, train_acc: 0.70
weight adjusted tensor(23.4517, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.63068, train_acc: 0.69
weight adjusted tensor(23.4459, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.58461, train_acc: 0.73
weight adjusted tensor(23.4410, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.74399, train_acc: 0.58
weight adjusted tensor(23.4429, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.76273, train_acc: 0.66
weight adjusted tensor(23.4421, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.56887, train_acc: 0.73
weight adjusted tensor(23.4385, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.60100, train_acc: 0.65
weight adjusted tensor(23.4367, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.61189, train_acc: 0.60
weight adjusted tensor(23.4361, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.61425, train_acc: 0.59
weight adjusted tensor(23.4362, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.61251, train_acc: 0.63
weight adjusted tensor(23.4370, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.60907, train_acc: 0.71
weight adjusted tensor(23.4382, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.60600, train_acc: 0.74
weight adjusted tensor(23.4411, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 0.60433, train_acc: 0.74
Finished Training
              precision    recall  f1-score   support

           0       0.59      0.50      0.54       132
           1       0.53      0.62      0.57       118

    accuracy                           0.56       250
   macro avg       0.56      0.56      0.56       250
weighted avg       0.56      0.56      0.55       250

input_torch torch.Size([750, 100])
sparsemans loss -2.6303745667165805
Epoch: 1, loss: -1.93723, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -2.6385665580291
Epoch: 2, loss: -1.94569, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -2.6418673659011804
Epoch: 3, loss: -1.95057, train_acc: 0.55
input_torch torch.Size([750, 100])
sparsemans loss -2.63217341118357
Epoch: 4, loss: -1.94130, train_acc: 0.53
input_torch torch.Size([750, 100])
sparsemans loss -2.6459512694413116
Epoch: 5, loss: -1.95682, train_acc: 0.57
input_torch torch.Size([750, 100])
sparsemans loss -2.6589551848942685
Epoch: 6, loss: -1.97138, train_acc: 0.52
input_torch torch.Size([750, 100])
sparsemans loss -2.634763479608386
Epoch: 7, loss: -1.94890, train_acc: 0.56
input_torch torch.Size([750, 100])
sparsemans loss -2.6227785677941453
Epoch: 8, loss: -1.93984, train_acc: 0.58
input_torch torch.Size([750, 100])
sparsemans loss -2.5847364150395107
Epoch: 9, loss: -1.90572, train_acc: 0.61
input_torch torch.Size([750, 100])
sparsemans loss -2.567436465553545
Epoch: 10, loss: -1.89340, train_acc: 0.59
input_torch torch.Size([750, 100])
sparsemans loss -2.5663825247566847
Epoch: 11, loss: -1.90029, train_acc: 0.62
input_torch torch.Size([750, 100])
sparsemans loss -2.513228602932099
Epoch: 12, loss: -1.85969, train_acc: 0.69
input_torch torch.Size([750, 100])
sparsemans loss -2.483230097322368
Epoch: 13, loss: -1.84930, train_acc: 0.70
input_torch torch.Size([750, 100])
sparsemans loss -2.446451578955138
Epoch: 14, loss: -1.85340, train_acc: 0.68
input_torch torch.Size([750, 100])
sparsemans loss -2.4362527597383132
Epoch: 15, loss: -1.41566, train_acc: 0.63
input_torch torch.Size([750, 100])
sparsemans loss -2.4327099189338166
Epoch: 16, loss: -1.34981, train_acc: 0.54
input_torch torch.Size([750, 100])
sparsemans loss -2.4376209586096635
Epoch: 17, loss: -1.67767, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -2.4461304288912866
Epoch: 18, loss: -1.79813, train_acc: 0.54
input_torch torch.Size([750, 100])
sparsemans loss -2.4493814328379075
Epoch: 19, loss: -1.82247, train_acc: 0.57
input_torch torch.Size([750, 100])
sparsemans loss -2.4338423680542403
Epoch: 20, loss: -1.80997, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss -2.3811910854322123
Epoch: 21, loss: -1.75564, train_acc: 0.72
input_torch torch.Size([750, 100])
sparsemans loss -2.3823406302883376
Epoch: 22, loss: -1.75364, train_acc: 0.76
input_torch torch.Size([750, 100])
sparsemans loss -2.363227934716187
Epoch: 23, loss: -1.73106, train_acc: 0.75
input_torch torch.Size([750, 100])
sparsemans loss -2.338473256918372
Epoch: 24, loss: -1.70321, train_acc: 0.71
input_torch torch.Size([750, 100])
sparsemans loss -2.3439602763047716
Epoch: 25, loss: -1.70662, train_acc: 0.70
Finished Training
              precision    recall  f1-score   support

           0       0.59      0.71      0.65       132
           1       0.58      0.45      0.51       118

    accuracy                           0.59       250
   macro avg       0.59      0.58      0.58       250
weighted avg       0.59      0.59      0.58       250

weight adjusted tensor(0.2347, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.45852, train_acc: 0.48
weight adjusted tensor(0.2347, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.46034, train_acc: 0.51
weight adjusted tensor(0.2347, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.45721, train_acc: 0.51
weight adjusted tensor(0.2347, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.45651, train_acc: 0.53
weight adjusted tensor(0.2347, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.45648, train_acc: 0.51
weight adjusted tensor(0.2346, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.45517, train_acc: 0.59
weight adjusted tensor(0.2346, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.45331, train_acc: 0.52
weight adjusted tensor(0.2346, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.45166, train_acc: 0.60
weight adjusted tensor(0.2345, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.45002, train_acc: 0.60
weight adjusted tensor(0.2345, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.44749, train_acc: 0.60
weight adjusted tensor(0.2345, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.44352, train_acc: 0.62
weight adjusted tensor(0.2345, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.43824, train_acc: 0.63
weight adjusted tensor(0.2345, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.43170, train_acc: 0.63
weight adjusted tensor(0.2345, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.42225, train_acc: 0.67
weight adjusted tensor(0.2344, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.40637, train_acc: 0.71
weight adjusted tensor(0.2344, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.38141, train_acc: 0.72
weight adjusted tensor(0.2343, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.33679, train_acc: 0.72
weight adjusted tensor(0.2343, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.66985, train_acc: 0.57
weight adjusted tensor(0.2343, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.31876, train_acc: 0.76
weight adjusted tensor(0.2342, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.35180, train_acc: 0.68
weight adjusted tensor(0.2342, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.34664, train_acc: 0.69
weight adjusted tensor(0.2341, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.34046, train_acc: 0.75
weight adjusted tensor(0.2341, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.33623, train_acc: 0.81
weight adjusted tensor(0.2341, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.33131, train_acc: 0.87
weight adjusted tensor(0.2340, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 0.32476, train_acc: 0.89
Finished Training
              precision    recall  f1-score   support

           0       0.63      0.48      0.54       132
           1       0.54      0.69      0.60       118

    accuracy                           0.58       250
   macro avg       0.58      0.58      0.57       250
weighted avg       0.59      0.58      0.57       250

sparsemans loss -0.26303745667165807
word analogy weight adjusted tensor(23.4717, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: -23.04127, train_acc: 0.49
sparsemans loss -0.2664184668939328
word analogy weight adjusted tensor(23.4741, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: -23.04596, train_acc: 0.51
sparsemans loss -0.2665236211208099
word analogy weight adjusted tensor(23.4785, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: -23.05299, train_acc: 0.51
sparsemans loss -0.2690580598052735
word analogy weight adjusted tensor(23.4841, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: -23.06213, train_acc: 0.51
sparsemans loss -0.26767907990034767
word analogy weight adjusted tensor(23.4864, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: -23.06312, train_acc: 0.58
sparsemans loss -0.2626874823337655
word analogy weight adjusted tensor(23.4877, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: -23.06061, train_acc: 0.58
sparsemans loss -0.2598579641891059
word analogy weight adjusted tensor(23.4880, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: -23.05985, train_acc: 0.52
sparsemans loss -0.25848139452733615
word analogy weight adjusted tensor(23.4880, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: -23.06027, train_acc: 0.58
sparsemans loss -0.25558809144634675
word analogy weight adjusted tensor(23.4882, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: -23.05939, train_acc: 0.59
sparsemans loss -0.25192571229713756
word analogy weight adjusted tensor(23.4887, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: -23.05887, train_acc: 0.58
sparsemans loss -0.2500737864695517
word analogy weight adjusted tensor(23.4907, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: -23.06318, train_acc: 0.60
sparsemans loss -0.24124928532543272
word analogy weight adjusted tensor(23.4917, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: -23.06100, train_acc: 0.65
sparsemans loss -0.23978426521008894
word analogy weight adjusted tensor(23.4927, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: -23.06777, train_acc: 0.68
sparsemans loss -0.23730515891370635
word analogy weight adjusted tensor(23.4929, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: -23.07630, train_acc: 0.71
sparsemans loss -0.23220485631444973
word analogy weight adjusted tensor(23.4935, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: -23.08997, train_acc: 0.72
sparsemans loss -0.22711866052163018
word analogy weight adjusted tensor(23.4911, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: -23.10908, train_acc: 0.68
sparsemans loss -0.2272081119688048
word analogy weight adjusted tensor(23.4881, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: -23.16349, train_acc: 0.75
sparsemans loss -0.2235931891597092
word analogy weight adjusted tensor(23.4862, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: -22.81484, train_acc: 0.66
sparsemans loss -0.22438053440979464
word analogy weight adjusted tensor(23.4841, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: -22.51675, train_acc: 0.52
sparsemans loss -0.22342037223182426
word analogy weight adjusted tensor(23.4729, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: -23.05497, train_acc: 0.61
sparsemans loss -0.2224729926204002
word analogy weight adjusted tensor(23.4622, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: -23.12101, train_acc: 0.78
sparsemans loss -0.22137560940050022
word analogy weight adjusted tensor(23.4521, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: -23.10390, train_acc: 0.84
sparsemans loss -0.22065120495152665
word analogy weight adjusted tensor(23.4442, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: -23.08564, train_acc: 0.83
sparsemans loss -0.21994875903295147
word analogy weight adjusted tensor(23.4379, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: -23.07084, train_acc: 0.75
sparsemans loss -0.2161708276558174
word analogy weight adjusted tensor(23.4328, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: -23.05640, train_acc: 0.70
Finished Training
              precision    recall  f1-score   support

           0       0.86      0.09      0.16       132
           1       0.49      0.98      0.66       118

    accuracy                           0.51       250
   macro avg       0.67      0.54      0.41       250
weighted avg       0.68      0.51      0.40       250

test_pad.shape: 250
test_pad[0].shape: 100
weight adjusted tensor(178.3759, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.69571, train_acc: 0.51
weight adjusted tensor(178.4526, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.68856, train_acc: 0.54
weight adjusted tensor(178.3736, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.68113, train_acc: 0.62
weight adjusted tensor(178.6073, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.67310, train_acc: 0.67
weight adjusted tensor(178.4506, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.66458, train_acc: 0.67
weight adjusted tensor(178.4504, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.65410, train_acc: 0.67
weight adjusted tensor(178.2941, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.64025, train_acc: 0.69
weight adjusted tensor(178.2935, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.62250, train_acc: 0.70
weight adjusted tensor(178.3698, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.59929, train_acc: 0.71
weight adjusted tensor(178.4450, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.56530, train_acc: 0.72
weight adjusted tensor(178.5193, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.52219, train_acc: 0.75
weight adjusted tensor(178.5936, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.59419, train_acc: 0.75
weight adjusted tensor(178.5915, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.62027, train_acc: 0.68
weight adjusted tensor(178.5902, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.52259, train_acc: 0.72
weight adjusted tensor(178.6679, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.47588, train_acc: 0.78
weight adjusted tensor(178.6678, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.48349, train_acc: 0.79
weight adjusted tensor(178.6675, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.50311, train_acc: 0.77
weight adjusted tensor(178.7445, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.50898, train_acc: 0.76
weight adjusted tensor(178.7428, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.49942, train_acc: 0.78
weight adjusted tensor(178.6625, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.48230, train_acc: 0.80
weight adjusted tensor(178.5819, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.46434, train_acc: 0.82
weight adjusted tensor(178.3449, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.44837, train_acc: 0.84
weight adjusted tensor(178.4986, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.43414, train_acc: 0.85
weight adjusted tensor(178.4182, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.41945, train_acc: 0.85
weight adjusted tensor(178.4180, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 0.40079, train_acc: 0.85
Finished Training
              precision    recall  f1-score   support

           0       0.78      0.66      0.72       132
           1       0.68      0.80      0.73       118

    accuracy                           0.72       250
   macro avg       0.73      0.73      0.72       250
weighted avg       0.73      0.72      0.72       250

input_torch torch.Size([750, 100])
sparsemans loss 13.753189739513823
Epoch: 1, loss: 14.44563, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 13.750750493279291
Epoch: 2, loss: 14.43641, train_acc: 0.59
input_torch torch.Size([750, 100])
sparsemans loss 13.751699126724285
Epoch: 3, loss: 14.42980, train_acc: 0.63
input_torch torch.Size([750, 100])
sparsemans loss 13.752029115109611
Epoch: 4, loss: 14.42170, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss 13.751670081982962
Epoch: 5, loss: 14.41175, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss 13.747731928560134
Epoch: 6, loss: 14.39526, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss 13.745854906224253
Epoch: 7, loss: 14.37653, train_acc: 0.68
input_torch torch.Size([750, 100])
sparsemans loss 13.749369171455172
Epoch: 8, loss: 14.35832, train_acc: 0.70
input_torch torch.Size([750, 100])
sparsemans loss 13.747849990735423
Epoch: 9, loss: 14.32654, train_acc: 0.71
input_torch torch.Size([750, 100])
sparsemans loss 13.747516703322747
Epoch: 10, loss: 14.28473, train_acc: 0.74
input_torch torch.Size([750, 100])
sparsemans loss 13.747033834745613
Epoch: 11, loss: 14.26968, train_acc: 0.75
input_torch torch.Size([750, 100])
sparsemans loss 13.738710760437309
Epoch: 12, loss: 14.60785, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss 13.739748215182155
Epoch: 13, loss: 14.21707, train_acc: 0.75
input_torch torch.Size([750, 100])
sparsemans loss 13.744372101417946
Epoch: 14, loss: 14.23812, train_acc: 0.77
input_torch torch.Size([750, 100])
sparsemans loss 13.743540606104597
Epoch: 15, loss: 14.27536, train_acc: 0.74
input_torch torch.Size([750, 100])
sparsemans loss 13.751223671260352
Epoch: 16, loss: 14.27713, train_acc: 0.75
input_torch torch.Size([750, 100])
sparsemans loss 13.753408514255671
Epoch: 17, loss: 14.26913, train_acc: 0.77
input_torch torch.Size([750, 100])
sparsemans loss 13.755476532019715
Epoch: 18, loss: 14.26606, train_acc: 0.79
input_torch torch.Size([750, 100])
sparsemans loss 13.75721443776589
Epoch: 19, loss: 14.26499, train_acc: 0.80
input_torch torch.Size([750, 100])
sparsemans loss 13.760080467154074
Epoch: 20, loss: 14.26505, train_acc: 0.81
input_torch torch.Size([750, 100])
sparsemans loss 13.751369920002134
Epoch: 21, loss: 14.25212, train_acc: 0.82
input_torch torch.Size([750, 100])
sparsemans loss 13.745011192052058
Epoch: 22, loss: 14.23906, train_acc: 0.83
input_torch torch.Size([750, 100])
sparsemans loss 13.746562082641363
Epoch: 23, loss: 14.23034, train_acc: 0.83
input_torch torch.Size([750, 100])
sparsemans loss 13.740122437012689
Epoch: 24, loss: 14.20873, train_acc: 0.84
input_torch torch.Size([750, 100])
sparsemans loss 13.740636326655283
Epoch: 25, loss: 14.18741, train_acc: 0.84
Finished Training
              precision    recall  f1-score   support

           0       0.70      0.67      0.68       132
           1       0.65      0.67      0.66       118

    accuracy                           0.67       250
   macro avg       0.67      0.67      0.67       250
weighted avg       0.67      0.67      0.67       250

weight adjusted tensor(1.7838, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: -1.09073, train_acc: 0.52
weight adjusted tensor(1.7822, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: -1.09741, train_acc: 0.63
weight adjusted tensor(1.7807, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: -1.10414, train_acc: 0.65
weight adjusted tensor(1.7814, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: -1.11417, train_acc: 0.66
weight adjusted tensor(1.7800, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: -1.12398, train_acc: 0.67
weight adjusted tensor(1.7808, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: -1.13903, train_acc: 0.68
weight adjusted tensor(1.7844, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: -1.16145, train_acc: 0.69
weight adjusted tensor(1.7836, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: -1.18697, train_acc: 0.71
weight adjusted tensor(1.7851, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: -1.22761, train_acc: 0.73
weight adjusted tensor(1.7859, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: -1.28511, train_acc: 0.75
weight adjusted tensor(1.7851, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: -1.07684, train_acc: 0.71
weight adjusted tensor(1.7851, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: -1.08954, train_acc: 0.73
weight adjusted tensor(1.7851, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: -1.22557, train_acc: 0.74
weight adjusted tensor(1.7867, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: -1.30811, train_acc: 0.78
weight adjusted tensor(1.7866, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: -1.30393, train_acc: 0.78
weight adjusted tensor(1.7875, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: -1.28886, train_acc: 0.77
weight adjusted tensor(1.7851, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: -1.27576, train_acc: 0.77
weight adjusted tensor(1.7851, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: -1.27128, train_acc: 0.77
weight adjusted tensor(1.7844, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: -1.27092, train_acc: 0.77
weight adjusted tensor(1.7836, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: -1.27406, train_acc: 0.77
weight adjusted tensor(1.7844, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: -1.28111, train_acc: 0.78
weight adjusted tensor(1.7844, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: -1.28891, train_acc: 0.80
weight adjusted tensor(1.7828, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: -1.29619, train_acc: 0.81
weight adjusted tensor(1.7805, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: -1.30348, train_acc: 0.83
weight adjusted tensor(1.7797, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: -1.31316, train_acc: 0.84
Finished Training
              precision    recall  f1-score   support

           0       0.71      0.68      0.70       132
           1       0.66      0.69      0.68       118

    accuracy                           0.69       250
   macro avg       0.69      0.69      0.69       250
weighted avg       0.69      0.69      0.69       250

sparsemans loss 1.3753189739513823
word analogy weight adjusted tensor(178.3759, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: -176.30637, train_acc: 0.48
sparsemans loss 1.3753334648363122
word analogy weight adjusted tensor(178.6098, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: -176.54808, train_acc: 0.58
sparsemans loss 1.3757121729625112
word analogy weight adjusted tensor(178.2064, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: -176.15214, train_acc: 0.63
sparsemans loss 1.3764439763757568
word analogy weight adjusted tensor(178.2845, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: -176.23822, train_acc: 0.65
sparsemans loss 1.3760961197845831
word analogy weight adjusted tensor(178.3622, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: -176.32693, train_acc: 0.65
sparsemans loss 1.3761112556526078
word analogy weight adjusted tensor(178.2834, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: -176.26159, train_acc: 0.67
sparsemans loss 1.376205439063234
word analogy weight adjusted tensor(178.3608, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: -176.35643, train_acc: 0.68
sparsemans loss 1.3757677443570209
word analogy weight adjusted tensor(178.5160, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: -176.53619, train_acc: 0.69
sparsemans loss 1.37507771568362
word analogy weight adjusted tensor(178.5928, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: -176.64783, train_acc: 0.71
sparsemans loss 1.375061153528503
word analogy weight adjusted tensor(178.5932, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: -176.69037, train_acc: 0.73
sparsemans loss 1.3751784113727428
word analogy weight adjusted tensor(178.6702, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: -176.45354, train_acc: 0.63
sparsemans loss 1.374974284620772
word analogy weight adjusted tensor(178.6691, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: -176.79897, train_acc: 0.76
sparsemans loss 1.375088567512454
word analogy weight adjusted tensor(178.7468, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: -176.81139, train_acc: 0.73
sparsemans loss 1.375858651113776
word analogy weight adjusted tensor(178.6306, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: -176.74550, train_acc: 0.77
sparsemans loss 1.376061494049215
word analogy weight adjusted tensor(178.4719, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: -176.58947, train_acc: 0.77
sparsemans loss 1.3759295789260624
word analogy weight adjusted tensor(178.5476, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: -176.65713, train_acc: 0.79
sparsemans loss 1.3757777545396968
word analogy weight adjusted tensor(178.3893, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: -176.49371, train_acc: 0.80
sparsemans loss 1.3759635095494036
word analogy weight adjusted tensor(178.5012, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: -176.60412, train_acc: 0.79
sparsemans loss 1.3759535713262403
word analogy weight adjusted tensor(178.4216, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: -176.52703, train_acc: 0.79
sparsemans loss 1.376064258273889
word analogy weight adjusted tensor(178.4204, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: -176.53151, train_acc: 0.81
sparsemans loss 1.3762408432413007
word analogy weight adjusted tensor(178.3413, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: -176.46117, train_acc: 0.82
sparsemans loss 1.3758922506713238
word analogy weight adjusted tensor(178.5748, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: -176.70700, train_acc: 0.83
sparsemans loss 1.3758965700342634
word analogy weight adjusted tensor(178.7304, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: -176.87761, train_acc: 0.83
sparsemans loss 1.3760791630053737
word analogy weight adjusted tensor(178.6518, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: -176.81710, train_acc: 0.84
sparsemans loss 1.3762202980187066
word analogy weight adjusted tensor(178.6513, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: -176.83851, train_acc: 0.85
Finished Training
              precision    recall  f1-score   support

           0       0.70      0.72      0.71       132
           1       0.68      0.65      0.66       118

    accuracy                           0.69       250
   macro avg       0.69      0.69      0.69       250
weighted avg       0.69      0.69      0.69       250

./embed/filtered.txt
=================================================================================
Serial              Dataset       Num Pairs       Not found             Rho
=================================================================================
     1         EN-MC-30.txt              30              0         0.7767
Vectors read from: ./embed/filtered.txt
     2     EN-MEN-TR-3k.txt            3000              0         0.7303
     3     EN-MTurk-287.txt             287              0         0.6580
     4     EN-MTurk-771.txt             771              0         0.6028
     5         EN-RG-65.txt              65              0         0.7715
     6   EN-RW-STANFORD.txt            2034             96         0.4093
     7    EN-SIMLEX-999.txt             999              0         0.3386
     8  EN-SimVerb-3500.txt            3500              2         0.2004
     9      EN-VERB-143.txt             144              0         0.3333
    10    EN-WS-353-ALL.txt             353              0         0.6570
    11    EN-WS-353-REL.txt             252              0         0.6027
    12    EN-WS-353-SIM.txt             203              0         0.7226
    13        EN-YP-130.txt             130              0         0.4274
./embed/filtered.txt
Vectors read from: ./embed/filtered.txt
test_pad.shape: 250
test_pad[0].shape: 100
weight adjusted tensor(1.9976, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.69339, train_acc: 0.49
weight adjusted tensor(1.9989, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.69783, train_acc: 0.51
weight adjusted tensor(1.9991, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.69253, train_acc: 0.51
weight adjusted tensor(1.9997, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.69219, train_acc: 0.51
weight adjusted tensor(2.0006, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.69304, train_acc: 0.50
weight adjusted tensor(2.0013, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.69195, train_acc: 0.51
weight adjusted tensor(2.0010, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.69014, train_acc: 0.56
weight adjusted tensor(2.0005, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.68883, train_acc: 0.55
weight adjusted tensor(2.0015, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.68816, train_acc: 0.52
weight adjusted tensor(2.0024, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.68744, train_acc: 0.51
weight adjusted tensor(2.0011, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.68591, train_acc: 0.51
weight adjusted tensor(1.9986, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.68361, train_acc: 0.55
weight adjusted tensor(1.9993, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.68097, train_acc: 0.62
weight adjusted tensor(2.0001, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.67811, train_acc: 0.66
weight adjusted tensor(2.0001, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.67457, train_acc: 0.67
weight adjusted tensor(1.9982, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.66945, train_acc: 0.68
weight adjusted tensor(2.0001, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.66226, train_acc: 0.68
weight adjusted tensor(2.0041, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.65376, train_acc: 0.68
weight adjusted tensor(2.0088, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.64096, train_acc: 0.73
weight adjusted tensor(2.0137, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.62206, train_acc: 0.81
weight adjusted tensor(2.0206, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.59422, train_acc: 0.89
weight adjusted tensor(2.0250, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.56292, train_acc: 0.66
weight adjusted tensor(2.0283, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 0.63955, train_acc: 0.57
weight adjusted tensor(2.0300, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 0.52273, train_acc: 0.82
weight adjusted tensor(2.0314, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 0.58574, train_acc: 0.80
Finished Training
              precision    recall  f1-score   support

           0       0.72      0.14      0.23       132
           1       0.49      0.94      0.65       118

    accuracy                           0.52       250
   macro avg       0.61      0.54      0.44       250
weighted avg       0.61      0.52      0.43       250

input_torch torch.Size([750, 100])
sparsemans loss -0.46959036161969436
Epoch: 1, loss: 0.22437, train_acc: 0.49
input_torch torch.Size([750, 100])
sparsemans loss -0.7524149078799574
Epoch: 2, loss: -0.05639, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.20882201609513532
Epoch: 3, loss: 0.90169, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.11635768966687285
Epoch: 4, loss: 0.80818, train_acc: 0.53
input_torch torch.Size([750, 100])
sparsemans loss 0.02580678499246339
Epoch: 5, loss: 0.71802, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss 0.02810375539996699
Epoch: 6, loss: 0.71978, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -0.06186418415962661
Epoch: 7, loss: 0.62845, train_acc: 0.55
input_torch torch.Size([750, 100])
sparsemans loss -0.11282331752899169
Epoch: 8, loss: 0.57620, train_acc: 0.56
input_torch torch.Size([750, 100])
sparsemans loss -0.065432279706157
Epoch: 9, loss: 0.62270, train_acc: 0.59
input_torch torch.Size([750, 100])
sparsemans loss -0.06901376864076955
Epoch: 10, loss: 0.61822, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -0.14824893164252312
Epoch: 11, loss: 0.53737, train_acc: 0.51
input_torch torch.Size([750, 100])
sparsemans loss -0.37607023433092124
Epoch: 12, loss: 0.30717, train_acc: 0.61
input_torch torch.Size([750, 100])
sparsemans loss -0.5998043335618083
Epoch: 13, loss: 0.08067, train_acc: 0.64
input_torch torch.Size([750, 100])
sparsemans loss -0.5844216732926636
Epoch: 14, loss: 0.09293, train_acc: 0.65
input_torch torch.Size([750, 100])
sparsemans loss -0.49719784765505215
Epoch: 15, loss: 0.17598, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss -0.39390955335475936
Epoch: 16, loss: 0.27323, train_acc: 0.66
input_torch torch.Size([750, 100])
sparsemans loss -0.38139373195106313
Epoch: 17, loss: 0.27824, train_acc: 0.67
input_torch torch.Size([750, 100])
sparsemans loss -0.32434990109024303
Epoch: 18, loss: 0.32521, train_acc: 0.69
input_torch torch.Size([750, 100])
sparsemans loss -0.14258025049714956
Epoch: 19, loss: 0.49219, train_acc: 0.73
input_torch torch.Size([750, 100])
sparsemans loss -0.18043474771446008
Epoch: 20, loss: 0.43376, train_acc: 0.78
input_torch torch.Size([750, 100])
sparsemans loss -0.31736975101867937
Epoch: 21, loss: 0.26401, train_acc: 0.84
input_torch torch.Size([750, 100])
sparsemans loss -0.20553236819010803
Epoch: 22, loss: 0.38319, train_acc: 0.71
input_torch torch.Size([750, 100])
sparsemans loss -0.31986370041196577
Epoch: 23, loss: 0.37848, train_acc: 0.52
input_torch torch.Size([750, 100])
sparsemans loss -0.2567654813344773
Epoch: 24, loss: 0.32663, train_acc: 0.58
input_torch torch.Size([750, 100])
sparsemans loss 0.29082253147888315
Epoch: 25, loss: 0.83993, train_acc: 0.76
Finished Training
              precision    recall  f1-score   support

           0       0.58      0.34      0.43       132
           1       0.50      0.73      0.59       118

    accuracy                           0.52       250
   macro avg       0.54      0.53      0.51       250
weighted avg       0.54      0.52      0.51       250

weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: 0.67396, train_acc: 0.49
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: 0.67693, train_acc: 0.51
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: 0.67278, train_acc: 0.51
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: 0.67189, train_acc: 0.53
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: 0.67247, train_acc: 0.50
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: 0.67163, train_acc: 0.52
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: 0.66998, train_acc: 0.54
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: 0.66862, train_acc: 0.53
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: 0.66779, train_acc: 0.61
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: 0.66682, train_acc: 0.52
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: 0.66504, train_acc: 0.52
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: 0.66254, train_acc: 0.61
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: 0.65978, train_acc: 0.65
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: 0.65665, train_acc: 0.67
weight adjusted tensor(0.0200, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: 0.65240, train_acc: 0.68
weight adjusted tensor(0.0201, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: 0.64626, train_acc: 0.68
weight adjusted tensor(0.0201, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: 0.63854, train_acc: 0.70
weight adjusted tensor(0.0201, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: 0.62903, train_acc: 0.72
weight adjusted tensor(0.0194, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: 0.61477, train_acc: 0.77
weight adjusted tensor(0.0194, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: 0.59605, train_acc: 0.81
weight adjusted tensor(0.0194, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: 0.56555, train_acc: 0.85
weight adjusted tensor(0.0195, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: 0.51203, train_acc: 0.89
weight adjusted tensor(0.0196, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: 1.01211, train_acc: 0.57
weight adjusted tensor(0.0196, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: 1.14305, train_acc: 0.50
weight adjusted tensor(0.0195, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: 1.00466, train_acc: 0.50
Finished Training
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
D:\Anaconda\exe\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.53      1.00      0.69       132
           1       0.00      0.00      0.00       118

    accuracy                           0.53       250
   macro avg       0.26      0.50      0.35       250
weighted avg       0.28      0.53      0.36       250

sparsemans loss -0.046959036161969434
word analogy weight adjusted tensor(1.9976, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 1, loss: -1.35127, train_acc: 0.50
sparsemans loss 0.0694456948424618
word analogy weight adjusted tensor(1.9978, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 2, loss: -1.23606, train_acc: 0.50
sparsemans loss 0.05611270748009868
word analogy weight adjusted tensor(2.0003, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 3, loss: -1.25153, train_acc: 0.52
sparsemans loss 0.053532482181970265
word analogy weight adjusted tensor(2.0026, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 4, loss: -1.25785, train_acc: 0.53
sparsemans loss 0.05864821510632916
word analogy weight adjusted tensor(2.0055, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 5, loss: -1.25603, train_acc: 0.52
sparsemans loss 0.06213560230750291
word analogy weight adjusted tensor(2.0089, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 6, loss: -1.25742, train_acc: 0.54
sparsemans loss 0.06533323422775537
word analogy weight adjusted tensor(2.0122, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 7, loss: -1.25857, train_acc: 0.55
sparsemans loss 0.05737386670257283
word analogy weight adjusted tensor(2.0135, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 8, loss: -1.26913, train_acc: 0.56
sparsemans loss 0.06212156329683734
word analogy weight adjusted tensor(2.0169, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 9, loss: -1.27024, train_acc: 0.59
sparsemans loss 0.0513535978469421
word analogy weight adjusted tensor(2.0168, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 10, loss: -1.28362, train_acc: 0.57
sparsemans loss 0.0584403476253656
word analogy weight adjusted tensor(2.0206, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 11, loss: -1.28411, train_acc: 0.59
sparsemans loss 0.03620206607602518
word analogy weight adjusted tensor(2.0242, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 12, loss: -1.31570, train_acc: 0.64
sparsemans loss 0.04026909084636745
word analogy weight adjusted tensor(2.0224, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 13, loss: -1.31715, train_acc: 0.65
sparsemans loss 0.03704796439312904
word analogy weight adjusted tensor(2.0242, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 14, loss: -1.33441, train_acc: 0.69
sparsemans loss 0.011069659807421351
word analogy weight adjusted tensor(2.0279, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 15, loss: -1.37957, train_acc: 0.62
sparsemans loss -0.010657132846659522
word analogy weight adjusted tensor(2.0307, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 16, loss: -1.41079, train_acc: 0.66
sparsemans loss -0.00646966574017683
word analogy weight adjusted tensor(2.0219, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 17, loss: -1.41960, train_acc: 0.57
sparsemans loss -0.01667824199610486
word analogy weight adjusted tensor(2.0221, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 18, loss: -1.39535, train_acc: 0.59
sparsemans loss -0.03090464298656384
word analogy weight adjusted tensor(2.0198, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 19, loss: -1.38471, train_acc: 0.56
sparsemans loss -0.029377478021622213
word analogy weight adjusted tensor(1.9706, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 20, loss: -1.35771, train_acc: 0.60
sparsemans loss -0.03148416085262921
word analogy weight adjusted tensor(1.9254, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 21, loss: -1.33594, train_acc: 0.64
sparsemans loss -0.029871115411546474
word analogy weight adjusted tensor(1.9207, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 22, loss: -1.32196, train_acc: 0.61
sparsemans loss -0.027977094520234738
word analogy weight adjusted tensor(1.9166, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 23, loss: -1.32976, train_acc: 0.65
sparsemans loss -0.017822463631160336
word analogy weight adjusted tensor(1.9128, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 24, loss: -1.33044, train_acc: 0.70
sparsemans loss -0.012351874282571073
word analogy weight adjusted tensor(1.9089, dtype=torch.float64, grad_fn=<DivBackward0>)
Epoch: 25, loss: -1.32964, train_acc: 0.75
Finished Training
              precision    recall  f1-score   support

           0       0.53      0.61      0.57       132
           1       0.48      0.41      0.44       118

    accuracy                           0.51       250
   macro avg       0.51      0.51      0.50       250
weighted avg       0.51      0.51      0.51       250


Process finished with exit code 0
